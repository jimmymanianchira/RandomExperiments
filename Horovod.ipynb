{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Horovod.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jimmymanianchira/RandomExperiments/blob/master/Horovod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp6u5xl26xNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.3-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1L6kjEQtjF5",
        "colab_type": "text"
      },
      "source": [
        "## Failed Experimentation of working with Horovod. Need to figure out why this failed\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw5vJ-7l16kB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install h5py >=2.9.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0JFxdQu2Ajg",
        "colab_type": "code",
        "outputId": "d9b23cae-d196-4aa6-c342-2ac03f340430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import h5py\n",
        "h5py.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.9.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV_seD3BC8Nl",
        "colab_type": "code",
        "outputId": "e50cc1c5-3a19-4cc9-f75a-53e723cc3021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "!pip install horovod\n",
        "!pip install pyspark"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: horovod in /usr/local/lib/python3.6/dist-packages (0.16.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from horovod) (1.12.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from horovod) (5.4.8)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from horovod) (0.6.1)\n",
            "Requirement already satisfied: cffi>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from horovod) (1.12.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.4.0->horovod) (2.19)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied: py4j==0.10.7 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-WAV_eW60yh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.3-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1s4-oZ9yf4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twExyIyo7CqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC0qWCVsGmY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark import SparkConf, Row\n",
        "import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGAmGvph7F42",
        "colab_type": "code",
        "outputId": "f1147fcf-6dd3-4743-9a2a-ddeb0fc74758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def fn(magic_number):\n",
        "  import horovod.torch as hvd\n",
        "  hvd.init()\n",
        "  print('Hello, rank = %d, local_rank = %d, size = %d, local_size = %d, magic_number = %d' % (hvd.rank(), hvd.local_rank(), hvd.size(), hvd.local_size(), magic_number))\n",
        "  return hvd.rank()\n",
        "..."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DugqNJ3CGzti",
        "colab_type": "code",
        "outputId": "552b1ac5-5fe4-4e24-8e8a-fa3ae7b8d0b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "import horovod.spark\n",
        "horovod.spark.run(fn, args=(42,))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running 2 processes (inferred from spark.default.parallelism)...\n",
            "[1,0]<stdout>:Hello, rank = 0, local_rank = 0, size = 2, local_size = 2, magic_number = 42\n",
            "[1,1]<stdout>:Hello, rank = 1, local_rank = 1, size = 2, local_size = 2, magic_number = 42\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[1b280e9a30cd:02080] PMIX ERROR: NO-PERMISSIONS in file src/dstore/pmix_esh.c at line 530\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c12pOiLLFhHj",
        "colab_type": "code",
        "outputId": "5a1f69e5-f349-44b6-ae22-11e58bbc5288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "googletrend.csv\n",
            "sample_submission.csv\n",
            "state_names.csv\n",
            "store.csv\n",
            "store_states.csv\n",
            "test.csv\n",
            "train.csv\n",
            "weather.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AUPET3eJQvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj9eM1-CGZi4",
        "colab_type": "code",
        "outputId": "fc08725d-35b0-403d-9432-91eb9dd0d6b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!wget http://files.fast.ai/part2/lesson14/rossmann.tgz "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-04 06:31:15--  http://files.fast.ai/part2/lesson14/rossmann.tgz\n",
            "Resolving files.fast.ai (files.fast.ai)... 67.205.15.147\n",
            "Connecting to files.fast.ai (files.fast.ai)|67.205.15.147|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7730448 (7.4M) [text/plain]\n",
            "Saving to: ‘rossmann.tgz.1’\n",
            "\n",
            "\rrossmann.tgz.1        0%[                    ]       0  --.-KB/s               \rrossmann.tgz.1      100%[===================>]   7.37M  48.4MB/s    in 0.2s    \n",
            "\n",
            "2019-06-04 06:31:16 (48.4 MB/s) - ‘rossmann.tgz.1’ saved [7730448/7730448]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doHKDR3CJTur",
        "colab_type": "code",
        "outputId": "1a16b844-e135-4280-de50-2c71877552a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "!tar zxvf rossmann.tgz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "googletrend.csv\n",
            "sample_submission.csv\n",
            "state_names.csv\n",
            "store.csv\n",
            "store_states.csv\n",
            "test.csv\n",
            "train.csv\n",
            "weather.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B04lp21SFQo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import os\n",
        "import pyarrow as pa\n",
        "from pyspark import SparkConf, Row\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0czzQIiGUKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOCAL_SUBMISSION_CSV = 'submission.csv'\n",
        "LOCAL_CHECKPOINT_FILE = 'checkpoint.h5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHVqOIaBCrMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LIGHT_PROCESSING_CLUSTER = None  # or 'spark://hostname:7077'\n",
        "TRAINING_CLUSTER = None  # or 'spark://hostname:7077'\n",
        "\n",
        "# The number of training processes.\n",
        "NUM_TRAINING_PROC = 4\n",
        "\n",
        "# Desired sampling rate.  Useful to set to low number (e.g. 0.01) to make sure\n",
        "# that end-to-end process works.\n",
        "SAMPLE_RATE = None  # or use 0.01\n",
        "\n",
        "# Batch size & learning rate to use.\n",
        "BATCH_SIZE = 100\n",
        "LR = 1e-4\n",
        "\n",
        "# HDFS driver to use with Petastorm.\n",
        "PETASTORM_HDFS_DRIVER = 'libhdfs'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZncelUMC6Zg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conf = SparkConf().setAppName('data_prep').set('spark.sql.shuffle.partitions', '16')\n",
        "if LIGHT_PROCESSING_CLUSTER:\n",
        "    conf.setMaster(LIGHT_PROCESSING_CLUSTER)\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk8Pe959VRUL",
        "colab_type": "code",
        "outputId": "cfa64807-72c5-433d-9d8b-08ed246345b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'=2.9.0'\t\t spark-2.4.3-bin-hadoop2.7\t   test.csv\n",
            " googletrend.csv\t spark-2.4.3-bin-hadoop2.7.tgz\t   test_df.parquet\n",
            " rossmann.tgz\t\t spark-2.4.3-bin-hadoop2.7.tgz.1   train.csv\n",
            " rossmann.tgz.1\t\t state_names.csv\t\t   train_df.parquet\n",
            " sample_data\t\t store.csv\t\t\t   val_df.parquet\n",
            " sample_submission.csv\t store_states.csv\t\t   weather.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us1Wr0OQI_sB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM7MbeVDVH9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_csv = spark.read.csv('train.csv', header=True)\n",
        "test_csv = spark.read.csv('test.csv', header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq8wovf8VXIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "store_csv = spark.read.csv('store.csv', header=True)\n",
        "store_states_csv = spark.read.csv('store_states.csv', header=True)\n",
        "state_names_csv = spark.read.csv('state_names.csv' , header=True)\n",
        "google_trend_csv = spark.read.csv('googletrend.csv' , header=True)\n",
        "weather_csv = spark.read.csv('weather.csv' , header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_DG21PCVnbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def expand_date(df):\n",
        "    df = df.withColumn('Date', df.Date.cast(T.DateType()))\n",
        "    return df \\\n",
        "        .withColumn('Year', F.year(df.Date)) \\\n",
        "        .withColumn('Month', F.month(df.Date)) \\\n",
        "        .withColumn('Week', F.weekofyear(df.Date)) \\\n",
        "        .withColumn('Day', F.dayofmonth(df.Date))\n",
        "\n",
        "\n",
        "def prepare_google_trend():\n",
        "    # Extract week start date and state.\n",
        "    google_trend_all = google_trend_csv \\\n",
        "        .withColumn('Date', F.regexp_extract(google_trend_csv.week, '(.*?) -', 1)) \\\n",
        "        .withColumn('State', F.regexp_extract(google_trend_csv.file, 'Rossmann_DE_(.*)', 1))\n",
        "\n",
        "    # Map state NI -> HB,NI to align with other data sources.\n",
        "    google_trend_all = google_trend_all \\\n",
        "        .withColumn('State', F.when(google_trend_all.State == 'NI', 'HB,NI').otherwise(google_trend_all.State))\n",
        "\n",
        "    # Expand dates.\n",
        "    return expand_date(google_trend_all)\n",
        "\n",
        "\n",
        "def add_elapsed(df, cols):\n",
        "    def add_elapsed_column(col, asc):\n",
        "        def fn(rows):\n",
        "            last_store, last_date = None, None\n",
        "            for r in rows:\n",
        "                if last_store != r.Store:\n",
        "                    last_store = r.Store\n",
        "                    last_date = r.Date\n",
        "                if r[col]:\n",
        "                    last_date = r.Date\n",
        "                fields = r.asDict().copy()\n",
        "                fields[('After' if asc else 'Before') + col] = (r.Date - last_date).days\n",
        "                yield Row(**fields)\n",
        "        return fn\n",
        "\n",
        "    df = df.repartition(df.Store)\n",
        "    for asc in [False, True]:\n",
        "        sort_col = df.Date.asc() if asc else df.Date.desc()\n",
        "        rdd = df.sortWithinPartitions(df.Store.asc(), sort_col).rdd\n",
        "        for col in cols:\n",
        "            rdd = rdd.mapPartitions(add_elapsed_column(col, asc))\n",
        "        df = rdd.toDF()\n",
        "    return df\n",
        "\n",
        "\n",
        "def prepare_df(df):\n",
        "    num_rows = df.count()\n",
        "\n",
        "    # Expand dates.\n",
        "    df = expand_date(df)\n",
        "\n",
        "    df = df \\\n",
        "        .withColumn('Open', df.Open != '0') \\\n",
        "        .withColumn('Promo', df.Promo != '0') \\\n",
        "        .withColumn('StateHoliday', df.StateHoliday != '0') \\\n",
        "        .withColumn('SchoolHoliday', df.SchoolHoliday != '0')\n",
        "\n",
        "    # Merge in store information.\n",
        "    store = store_csv.join(store_states_csv, 'Store')\n",
        "    df = df.join(store, 'Store')\n",
        "\n",
        "    # Merge in Google Trend information.\n",
        "    google_trend_all = prepare_google_trend()\n",
        "    df = df.join(google_trend_all, ['State', 'Year', 'Week']).select(df['*'], google_trend_all.trend)\n",
        "\n",
        "    # Merge in Google Trend for whole Germany.\n",
        "    google_trend_de = google_trend_all[google_trend_all.file == 'Rossmann_DE']\n",
        "    df = df.join(google_trend_de, ['Year', 'Week']).select(df['*'], google_trend_all.trend.alias('trend_de'))\n",
        "\n",
        "    # Merge in weather.\n",
        "    weather = weather_csv.join(state_names_csv, weather_csv.file == state_names_csv.StateName)\n",
        "    df = df.join(weather, ['State', 'Date'])\n",
        "\n",
        "    # Fix null values.\n",
        "    df = df \\\n",
        "        .withColumn('CompetitionOpenSinceYear', F.coalesce(df.CompetitionOpenSinceYear, F.lit(1900))) \\\n",
        "        .withColumn('CompetitionOpenSinceMonth', F.coalesce(df.CompetitionOpenSinceMonth, F.lit(1))) \\\n",
        "        .withColumn('Promo2SinceYear', F.coalesce(df.Promo2SinceYear, F.lit(1900))) \\\n",
        "        .withColumn('Promo2SinceWeek', F.coalesce(df.Promo2SinceWeek, F.lit(1)))\n",
        "\n",
        "    # Days & months competition was open, cap to 2 years.\n",
        "    df = df.withColumn('CompetitionOpenSince',\n",
        "                       F.to_date(F.format_string('%s-%s-15', df.CompetitionOpenSinceYear,\n",
        "                                                 df.CompetitionOpenSinceMonth)))\n",
        "    df = df.withColumn('CompetitionDaysOpen',\n",
        "                       F.when(df.CompetitionOpenSinceYear > 1900,\n",
        "                              F.greatest(F.lit(0), F.least(F.lit(360 * 2), F.datediff(df.Date, df.CompetitionOpenSince))))\n",
        "                       .otherwise(0))\n",
        "    df = df.withColumn('CompetitionMonthsOpen', (df.CompetitionDaysOpen / 30).cast(T.IntegerType()))\n",
        "\n",
        "    # Days & weeks of promotion, cap to 25 weeks.\n",
        "    df = df.withColumn('Promo2Since',\n",
        "                       F.expr('date_add(format_string(\"%s-01-01\", Promo2SinceYear), (Promo2SinceWeek - 1) * 7)'))\n",
        "    df = df.withColumn('Promo2Days',\n",
        "                       F.when(df.Promo2SinceYear > 1900,\n",
        "                              F.greatest(F.lit(0), F.least(F.lit(25 * 7), F.datediff(df.Date, df.Promo2Since))))\n",
        "                       .otherwise(0))\n",
        "    df = df.withColumn('Promo2Weeks', (df.Promo2Days / 7).cast(T.IntegerType()))\n",
        "\n",
        "    # Check that we did not lose any rows through inner joins.\n",
        "    assert num_rows == df.count(), 'lost rows in joins'\n",
        "    return df\n",
        "\n",
        "\n",
        "def build_vocabulary(df, cols):\n",
        "    vocab = {}\n",
        "    for col in cols:\n",
        "        values = [r[0] for r in df.select(col).distinct().collect()]\n",
        "        col_type = type([x for x in values if x is not None][0])\n",
        "        default_value = col_type()\n",
        "        vocab[col] = sorted(values, key=lambda x: x or default_value)\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def cast_columns(df, cols):\n",
        "    for col in cols:\n",
        "        df = df.withColumn(col, F.coalesce(df[col].cast(T.FloatType()), F.lit(0.0)))\n",
        "    return df\n",
        "\n",
        "\n",
        "def lookup_columns(df, vocab):\n",
        "    def lookup(mapping):\n",
        "        def fn(v):\n",
        "            return mapping.index(v)\n",
        "        return F.udf(fn, returnType=T.IntegerType())\n",
        "\n",
        "    for col, mapping in vocab.items():\n",
        "        df = df.withColumn(col, lookup(mapping)(df[col]))\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILz8B-2JVw4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if SAMPLE_RATE:\n",
        "    train_csv = train_csv.sample(withReplacement=False, fraction=SAMPLE_RATE)\n",
        "    test_csv = test_csv.sample(withReplacement=False, fraction=SAMPLE_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZYL8fTjVyvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = prepare_df(train_csv).cache()\n",
        "test_df = prepare_df(test_csv).cache()\n",
        "\n",
        "# Add elapsed times from holidays & promos, the data spanning training & test datasets.\n",
        "elapsed_cols = ['Promo', 'StateHoliday', 'SchoolHoliday']\n",
        "elapsed = add_elapsed(train_df.select('Date', 'Store', *elapsed_cols)\n",
        "                              .unionAll(test_df.select('Date', 'Store', *elapsed_cols)),\n",
        "                      elapsed_cols)\n",
        "\n",
        "# Join with elapsed times.\n",
        "train_df = train_df \\\n",
        "    .join(elapsed, ['Date', 'Store']) \\\n",
        "    .select(train_df['*'], *[prefix + col for prefix in ['Before', 'After'] for col in elapsed_cols])\n",
        "test_df = test_df \\\n",
        "    .join(elapsed, ['Date', 'Store']) \\\n",
        "    .select(test_df['*'], *[prefix + col for prefix in ['Before', 'After'] for col in elapsed_cols])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5lo8CaCV1xO",
        "colab_type": "code",
        "outputId": "a82282a5-9ea6-4ff1-e250-9af06185fab9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        }
      },
      "source": [
        "train_df = train_df.filter(train_df.Sales > 0)\n",
        "\n",
        "print('===================')\n",
        "print('Prepared data frame')\n",
        "print('===================')\n",
        "train_df.show()\n",
        "\n",
        "categorical_cols = [\n",
        "    'Store', 'State', 'DayOfWeek', 'Year', 'Month', 'Day', 'Week', 'CompetitionMonthsOpen', 'Promo2Weeks', 'StoreType',\n",
        "    'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear', 'Events', 'Promo',\n",
        "    'StateHoliday', 'SchoolHoliday'\n",
        "]\n",
        "\n",
        "continuous_cols = [\n",
        "    'CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC', 'Max_Humidity',\n",
        "    'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', 'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
        "    'BeforePromo', 'AfterPromo', 'AfterStateHoliday', 'BeforeStateHoliday', 'BeforeSchoolHoliday', 'AfterSchoolHoliday'\n",
        "]\n",
        "\n",
        "all_cols = categorical_cols + continuous_cols"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===================\n",
            "Prepared data frame\n",
            "===================\n",
            "+-----+----------+-----+---------+-----+---------+----+-----+------------+-------------+----+-----+----+---+---------+----------+-------------------+-------------------------+------------------------+------+---------------+---------------+----------------+-----+--------+------------------+----------------+-----------------+----------------+----------+--------------+-------------+------------+-------------+------------+-------------------------+--------------------------+-------------------------+----------------+-----------------+----------------+------------------+-------------------+------------------+---------------+----------+--------+--------------+------------------+--------------------+-------------------+---------------------+-----------+----------+-----------+-----------+------------------+-------------------+----------+-----------------+------------------+\n",
            "|State|      Date|Store|DayOfWeek|Sales|Customers|Open|Promo|StateHoliday|SchoolHoliday|Year|Month|Week|Day|StoreType|Assortment|CompetitionDistance|CompetitionOpenSinceMonth|CompetitionOpenSinceYear|Promo2|Promo2SinceWeek|Promo2SinceYear|   PromoInterval|trend|trend_de|              file|Max_TemperatureC|Mean_TemperatureC|Min_TemperatureC|Dew_PointC|MeanDew_PointC|Min_DewpointC|Max_Humidity|Mean_Humidity|Min_Humidity|Max_Sea_Level_PressurehPa|Mean_Sea_Level_PressurehPa|Min_Sea_Level_PressurehPa|Max_VisibilityKm|Mean_VisibilityKm|Min_VisibilitykM|Max_Wind_SpeedKm_h|Mean_Wind_SpeedKm_h|Max_Gust_SpeedKm_h|Precipitationmm|CloudCover|  Events|WindDirDegrees|         StateName|CompetitionOpenSince|CompetitionDaysOpen|CompetitionMonthsOpen|Promo2Since|Promo2Days|Promo2Weeks|BeforePromo|BeforeStateHoliday|BeforeSchoolHoliday|AfterPromo|AfterStateHoliday|AfterSchoolHoliday|\n",
            "+-----+----------+-----+---------+-----+---------+----+-----+------------+-------------+----+-----+----+---+---------+----------+-------------------+-------------------------+------------------------+------+---------------+---------------+----------------+-----+--------+------------------+----------------+-----------------+----------------+----------+--------------+-------------+------------+-------------+------------+-------------------------+--------------------------+-------------------------+----------------+-----------------+----------------+------------------+-------------------+------------------+---------------+----------+--------+--------------+------------------+--------------------+-------------------+---------------------+-----------+----------+-----------+-----------+------------------+-------------------+----------+-----------------+------------------+\n",
            "|   BE|2013-01-02| 1002|        3| 4867|      676|true|false|       false|         true|2013|    1|   1|  2|        d|         c|               1130|                       11|                    2008|     0|              1|           1900|            null|   70|      70|            Berlin|               6|                4|               3|         4|             3|            1|          93|           86|          73|                     1024|                      1017|                     1009|              31|               15|              10|                29|                 19|                NA|           0.51|         6|    Rain|           255|            Berlin|          2008-11-15|                720|                   24| 1900-01-01|         0|          0|         -5|               -86|                  0|         1|                1|                 0|\n",
            "|   BE|2013-01-02| 1021|        3| 8903|     1075|true|false|       false|         true|2013|    1|   1|  2|        a|         a|               1080|                        5|                    2011|     0|              1|           1900|            null|   70|      70|            Berlin|               6|                4|               3|         4|             3|            1|          93|           86|          73|                     1024|                      1017|                     1009|              31|               15|              10|                29|                 19|                NA|           0.51|         6|    Rain|           255|            Berlin|          2011-05-15|                598|                   19| 1900-01-01|         0|          0|         -5|               -86|                  0|         1|                1|                 0|\n",
            "|   BY|2013-01-02| 1027|        3| 9515|     1510|true|false|       false|         true|2013|    1|   1|  2|        a|         c|                190|                        6|                    2008|     1|             40|           2011| Jan,Apr,Jul,Oct|   56|      56|            Bayern|               4|                1|              -3|         3|             1|           -3|         100|           88|          75|                     1031|                      1024|                     1014|              10|               10|              10|                19|                 13|                NA|              0|         6|Fog-Rain|           253|            Bayern|          2008-06-15|                720|                   24| 2011-10-01|       175|         25|         -5|                -4|                  0|         1|                1|                 0|\n",
            "|   NW|2013-01-02| 1031|        3| 4392|      424|true|false|       false|         true|2013|    1|   1|  2|        d|         a|                590|                        5|                    2001|     0|              1|           1900|            null|   63|      63|NordrheinWestfalen|               7|                4|               1|         5|             3|            2|          93|           85|          78|                     1028|                      1022|                     1014|              31|               14|              10|                24|                 16|                NA|              0|         6|    Rain|           225|NordrheinWestfalen|          2001-05-15|                720|                   24| 1900-01-01|         0|          0|         -5|               -86|                  0|         1|                1|                 0|\n",
            "|   ST|2013-01-02| 1046|        3| 6832|      614|true|false|       false|         true|2013|    1|   1|  2|        d|         c|              29070|                        4|                    2005|     0|              1|           1900|            null|   72|      72|     SachsenAnhalt|               7|                4|               3|         4|             2|            1|          81|           75|          66|                     1026|                      1018|                     1011|              31|               25|              14|                18|                 13|                NA|           0.25|         8|    Rain|            -1|     SachsenAnhalt|          2005-04-15|                720|                   24| 1900-01-01|         0|          0|         -5|                -4|                  0|         1|                1|                 0|\n",
            "|   BW|2013-01-02| 1068|        3| 3185|      267|true|false|       false|         true|2013|    1|   1|  2|        d|         c|               5010|                        1|                    1900|     1|              5|           2013| Jan,Apr,Jul,Oct|   59|      59| BadenWuerttemberg|               6|                3|              -1|         3|            -1|           -3|          93|           77|          52|                     1033|                      1024|                     1016|              31|               10|              10|                19|                 10|                NA|              0|         4|    Rain|           248| BadenWuerttemberg|          1900-01-15|                  0|                    0| 2013-01-29|         0|          0|         -5|                -4|                  0|         1|                1|                 0|\n",
            "|   NW|2013-01-02| 1073|        3| 6613|      903|true|false|       false|         true|2013|    1|   1|  2|        a|         c|               1710|                        1|                    1900|     1|             44|           2012| Jan,Apr,Jul,Oct|   63|      63|NordrheinWestfalen|               7|                4|               1|         5|             3|            2|          93|           85|          78|                     1028|                      1022|                     1014|              31|               14|              10|                24|                 16|                NA|              0|         6|    Rain|           225|NordrheinWestfalen|          1900-01-15|                  0|                    0| 2012-10-28|        66|          9|         -5|               -86|                  0|         1|                1|                 0|\n",
            "|   HE|2013-01-02|  182|        3| 3700|      319|true|false|       false|         true|2013|    1|   1|  2|        d|         c|               1390|                        1|                    1900|     1|              9|           2011|Mar,Jun,Sept,Dec|   73|      73|            Hessen|               6|                3|               0|         3|             1|           -1|         100|           85|          67|                     1031|                      1024|                     1015|              31|               13|              10|                19|                 11|                37|              0|         6|    null|           200|            Hessen|          1900-01-15|                  0|                    0| 2011-02-26|       175|         25|         -5|               -86|                  0|         1|                1|                 0|\n",
            "|   SH|2013-01-02|  187|        3| 5763|      755|true|false|       false|         true|2013|    1|   1|  2|        a|         c|              19360|                        1|                    1900|     0|              1|           1900|            null|   72|      72| SchleswigHolstein|               7|                6|               4|         4|             3|            2|          93|           86|          81|                     1021|                      1016|                     1010|              10|               10|              10|                26|                 21|                NA|              0|         4|    null|           266| SchleswigHolstein|          1900-01-15|                  0|                    0| 1900-01-01|         0|          0|         -5|               -86|                  0|         1|                1|                 0|\n",
            "|   BY|2013-01-02|  190|        3| 6927|      654|true|false|       false|         true|2013|    1|   1|  2|        a|         a|               1470|                       12|                    2006|     1|             40|           2014| Jan,Apr,Jul,Oct|   56|      56|            Bayern|               4|                1|              -3|         3|             1|           -3|         100|           88|          75|                     1031|                      1024|                     1014|              10|               10|              10|                19|                 13|                NA|              0|         6|Fog-Rain|           253|            Bayern|          2006-12-15|                720|                   24| 2014-10-01|         0|          0|         -5|                -4|                  0|         1|                1|                 0|\n",
            "|   HE|2013-01-02|  242|        3| 3444|      344|true|false|       false|         true|2013|    1|   1|  2|        d|         a|               6880|                        9|                    2001|     1|             14|           2011| Jan,Apr,Jul,Oct|   73|      73|            Hessen|               6|                3|               0|         3|             1|           -1|         100|           85|          67|                     1031|                      1024|                     1015|              31|               13|              10|                19|                 11|                37|              0|         6|    null|           200|            Hessen|          2001-09-15|                720|                   24| 2011-04-02|       175|         25|         -5|               -86|                  0|         1|                1|                 0|\n",
            "|   NW|2013-01-02|   25|        3|11944|     1698|true|false|       false|         true|2013|    1|   1|  2|        c|         a|                430|                        4|                    2003|     0|              1|           1900|            null|   63|      63|NordrheinWestfalen|               7|                4|               1|         5|             3|            2|          93|           85|          78|                     1028|                      1022|                     1014|              31|               14|              10|                24|                 16|                NA|              0|         6|    Rain|           225|NordrheinWestfalen|          2003-04-15|                720|                   24| 1900-01-01|         0|          0|         -5|               -86|                  0|         1|                1|                 0|\n",
            "|   NW|2013-01-02|  252|        3| 9661|      804|true|false|       false|         true|2013|    1|   1|  2|        d|         c|              22330|                        1|                    1900|     1|              5|           2010| Feb,May,Aug,Nov|   63|      63|NordrheinWestfalen|               7|                4|               1|         5|             3|            2|          93|           85|          78|                     1028|                      1022|                     1014|              31|               14|              10|                24|                 16|                NA|              0|         6|    Rain|           225|NordrheinWestfalen|          1900-01-15|                  0|                    0| 2010-01-29|       175|         25|         -5|               -86|                  0|         1|                1|                 0|\n",
            "|   BY|2013-01-02|  263|        3| 2961|      260|true|false|       false|         true|2013|    1|   1|  2|        a|         c|               1140|                        5|                    2013|     1|             40|           2014| Jan,Apr,Jul,Oct|   56|      56|            Bayern|               4|                1|              -3|         3|             1|           -3|         100|           88|          75|                     1031|                      1024|                     1014|              10|               10|              10|                19|                 13|                NA|              0|         6|Fog-Rain|           253|            Bayern|          2013-05-15|                  0|                    0| 2014-10-01|         0|          0|         -5|                -4|                  0|         1|                1|                 0|\n",
            "|   SN|2013-01-02|  271|        3| 5791|      759|true|false|       false|         true|2013|    1|   1|  2|        a|         a|                420|                        1|                    1900|     1|             14|           2011| Jan,Apr,Jul,Oct|   71|      71|           Sachsen|               5|                3|               2|         4|             1|           -1|          93|           78|          60|                     1028|                      1019|                     1010|              31|               13|              10|                29|                 18|                40|              0|         6|    Rain|           251|           Sachsen|          1900-01-15|                  0|                    0| 2011-04-02|       175|         25|         -5|               -86|                  0|         1|                1|                 0|\n",
            "|   TH|2013-01-02|  313|        3| 5591|      666|true|false|       false|         true|2013|    1|   1|  2|        d|         c|              14160|                        1|                    1900|     0|              1|           1900|            null|   65|      65|        Thueringen|               5|                3|               1|         2|             1|           -1|          93|           80|          67|                     1029|                      1020|                     1012|              10|               10|              10|                32|                 19|                NA|              0|         5|    Rain|           233|        Thueringen|          1900-01-15|                  0|                    0| 1900-01-01|         0|          0|         -5|               -86|                  0|         1|                1|                 0|\n",
            "|   TH|2013-01-02|  322|        3| 4116|      526|true|false|       false|         true|2013|    1|   1|  2|        a|         a|              17500|                        4|                    2001|     1|             37|           2009| Jan,Apr,Jul,Oct|   65|      65|        Thueringen|               5|                3|               1|         2|             1|           -1|          93|           80|          67|                     1029|                      1020|                     1012|              10|               10|              10|                32|                 19|                NA|              0|         5|    Rain|           233|        Thueringen|          2001-04-15|                720|                   24| 2009-09-10|       175|         25|         -5|               -86|                  0|         1|                1|                 0|\n",
            "|   RP|2013-01-02|  323|        3| 6425|      592|true|false|       false|         true|2013|    1|   1|  2|        d|         c|               8400|                        4|                    2012|     1|              5|           2013| Feb,May,Aug,Nov|   36|      36|    RheinlandPfalz|               7|                3|              -1|         3|             1|           -1|         100|           85|          67|                     1032|                      1025|                     1016|              31|               14|               6|                18|                 10|                NA|              0|         6|    null|           205|    RheinlandPfalz|          2012-04-15|                262|                    8| 2013-01-29|         0|          0|         -5|               -86|                  0|         1|                1|                 0|\n",
            "|   NW|2013-01-02|  328|        3| 4363|      571|true|false|       false|         true|2013|    1|   1|  2|        a|         a|               3130|                        7|                    2002|     0|              1|           1900|            null|   63|      63|NordrheinWestfalen|               7|                4|               1|         5|             3|            2|          93|           85|          78|                     1028|                      1022|                     1014|              31|               14|              10|                24|                 16|                NA|              0|         6|    Rain|           225|NordrheinWestfalen|          2002-07-15|                720|                   24| 1900-01-01|         0|          0|         -5|               -86|                  0|         1|                1|                 0|\n",
            "|   NW|2013-01-02|  358|        3| 8887|     1138|true|false|       false|         true|2013|    1|   1|  2|        a|         a|               2890|                       10|                    2003|     0|              1|           1900|            null|   63|      63|NordrheinWestfalen|               7|                4|               1|         5|             3|            2|          93|           85|          78|                     1028|                      1022|                     1014|              31|               14|              10|                24|                 16|                NA|              0|         6|    Rain|           225|NordrheinWestfalen|          2003-10-15|                720|                   24| 1900-01-01|         0|          0|         -5|               -86|                  0|         1|                1|                 0|\n",
            "+-----+----------+-----+---------+-----+---------+----+-----+------------+-------------+----+-----+----+---+---------+----------+-------------------+-------------------------+------------------------+------+---------------+---------------+----------------+-----+--------+------------------+----------------+-----------------+----------------+----------+--------------+-------------+------------+-------------+------------+-------------------------+--------------------------+-------------------------+----------------+-----------------+----------------+------------------+-------------------+------------------+---------------+----------+--------+--------------+------------------+--------------------+-------------------+---------------------+-----------+----------+-----------+-----------+------------------+-------------------+----------+-----------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQEari7AV5N-",
        "colab_type": "code",
        "outputId": "9f16eff9-1a6a-4c76-a46b-7017cdea4d80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        }
      },
      "source": [
        "train_df = train_df.select(*(all_cols + ['Sales', 'Date'])).cache()\n",
        "test_df = test_df.select(*(all_cols + ['Id', 'Date'])).cache()\n",
        "\n",
        "# Build vocabulary of categorical columns.\n",
        "vocab = build_vocabulary(train_df.select(*categorical_cols)\n",
        "                                 .unionAll(test_df.select(*categorical_cols)).cache(),\n",
        "                         categorical_cols)\n",
        "\n",
        "# Cast continuous columns to float & lookup categorical columns.\n",
        "train_df = cast_columns(train_df, continuous_cols + ['Sales'])\n",
        "train_df = lookup_columns(train_df, vocab)\n",
        "test_df = cast_columns(test_df, continuous_cols)\n",
        "test_df = lookup_columns(test_df, vocab)\n",
        "\n",
        "# Split into training & validation.\n",
        "# Test set is in 2015, use the same period in 2014 from the training set as a validation set.\n",
        "test_min_date = test_df.agg(F.min(test_df.Date)).collect()[0][0]\n",
        "test_max_date = test_df.agg(F.max(test_df.Date)).collect()[0][0]\n",
        "a_year = datetime.timedelta(365)\n",
        "val_df = train_df.filter((test_min_date - a_year <= train_df.Date) & (train_df.Date < test_max_date - a_year))\n",
        "train_df = train_df.filter((train_df.Date < test_min_date - a_year) | (train_df.Date >= test_max_date - a_year))\n",
        "\n",
        "# Determine max Sales number.\n",
        "max_sales = train_df.agg(F.max(train_df.Sales)).collect()[0][0]\n",
        "\n",
        "print('===================================')\n",
        "print('Data frame with transformed columns')\n",
        "print('===================================')\n",
        "train_df.show()\n",
        "\n",
        "print('================')\n",
        "print('Data frame sizes')\n",
        "print('================')\n",
        "train_rows, val_rows, test_rows = train_df.count(), val_df.count(), test_df.count()\n",
        "print('Training: %d' % train_rows)\n",
        "print('Validation: %d' % val_rows)\n",
        "print('Test: %d' % test_rows)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===================================\n",
            "Data frame with transformed columns\n",
            "===================================\n",
            "+-----+-----+---------+----+-----+---+----+---------------------+-----------+---------+----------+-------------+------------------------+---------------+------+-----+------------+-------------+-------------------+----------------+-----------------+----------------+------------+-------------+------------+------------------+-------------------+----------+-----+--------+-----------+----------+-----------------+------------------+-------------------+------------------+-------+----------+\n",
            "|Store|State|DayOfWeek|Year|Month|Day|Week|CompetitionMonthsOpen|Promo2Weeks|StoreType|Assortment|PromoInterval|CompetitionOpenSinceYear|Promo2SinceYear|Events|Promo|StateHoliday|SchoolHoliday|CompetitionDistance|Max_TemperatureC|Mean_TemperatureC|Min_TemperatureC|Max_Humidity|Mean_Humidity|Min_Humidity|Max_Wind_SpeedKm_h|Mean_Wind_SpeedKm_h|CloudCover|trend|trend_DE|BeforePromo|AfterPromo|AfterStateHoliday|BeforeStateHoliday|BeforeSchoolHoliday|AfterSchoolHoliday|  Sales|      Date|\n",
            "+-----+-----+---------+----+-----+---+----+---------------------+-----------+---------+----------+-------------+------------------------+---------------+------+-----+------------+-------------+-------------------+----------------+-----------------+----------------+------------+-------------+------------+------------------+-------------------+----------+-----+--------+-----------+----------+-----------------+------------------+-------------------+------------------+-------+----------+\n",
            "|    5|    0|        2|   0|    0|  1|   0|                   24|          0|        3|         2|            0|                      15|              0|    11|    0|           0|            1|             1130.0|             6.0|              4.0|             3.0|        93.0|         86.0|        73.0|              29.0|               19.0|       6.0| 70.0|    70.0|       -5.0|       1.0|              1.0|             -86.0|                0.0|               0.0| 4867.0|2013-01-02|\n",
            "|   26|    0|        2|   0|    0|  1|   0|                   19|          0|        0|         0|            0|                      18|              0|    11|    0|           0|            1|             1080.0|             6.0|              4.0|             3.0|        93.0|         86.0|        73.0|              29.0|               19.0|       6.0| 70.0|    70.0|       -5.0|       1.0|              1.0|             -86.0|                0.0|               0.0| 8903.0|2013-01-02|\n",
            "|   32|    2|        2|   0|    0|  1|   0|                   24|         25|        0|         2|            2|                      15|              3|     2|    0|           0|            1|              190.0|             4.0|              1.0|            -3.0|       100.0|         88.0|        75.0|              19.0|               13.0|       6.0| 56.0|    56.0|       -5.0|       1.0|              1.0|              -4.0|                0.0|               0.0| 9515.0|2013-01-02|\n",
            "|   37|    6|        2|   0|    0|  1|   0|                   24|          0|        3|         0|            0|                       8|              0|    11|    0|           0|            1|              590.0|             7.0|              4.0|             1.0|        93.0|         85.0|        78.0|              24.0|               16.0|       6.0| 63.0|    63.0|       -5.0|       1.0|              1.0|             -86.0|                0.0|               0.0| 4392.0|2013-01-02|\n",
            "|   53|   10|        2|   0|    0|  1|   0|                   24|          0|        3|         2|            0|                      12|              0|    11|    0|           0|            1|            29070.0|             7.0|              4.0|             3.0|        81.0|         75.0|        66.0|              18.0|               13.0|       8.0| 72.0|    72.0|       -5.0|       1.0|              1.0|              -4.0|                0.0|               0.0| 6832.0|2013-01-02|\n",
            "|   77|    1|        2|   0|    0|  1|   0|                    0|          0|        3|         2|            2|                       0|              5|    11|    0|           0|            1|             5010.0|             6.0|              3.0|            -1.0|        93.0|         77.0|        52.0|              19.0|               10.0|       4.0| 59.0|    59.0|       -5.0|       1.0|              1.0|              -4.0|                0.0|               0.0| 3185.0|2013-01-02|\n",
            "|   83|    6|        2|   0|    0|  1|   0|                    0|          9|        0|         2|            2|                       0|              4|    11|    0|           0|            1|             1710.0|             7.0|              4.0|             1.0|        93.0|         85.0|        78.0|              24.0|               16.0|       6.0| 63.0|    63.0|       -5.0|       1.0|              1.0|             -86.0|                0.0|               0.0| 6613.0|2013-01-02|\n",
            "|  208|    4|        2|   0|    0|  1|   0|                    0|         25|        3|         2|            3|                       0|              3|     0|    0|           0|            1|             1390.0|             6.0|              3.0|             0.0|       100.0|         85.0|        67.0|              19.0|               11.0|       6.0| 73.0|    73.0|       -5.0|       1.0|              1.0|             -86.0|                0.0|               0.0| 3700.0|2013-01-02|\n",
            "|  213|    8|        2|   0|    0|  1|   0|                    0|          0|        0|         2|            0|                       0|              0|     0|    0|           0|            1|            19360.0|             7.0|              6.0|             4.0|        93.0|         86.0|        81.0|              26.0|               21.0|       4.0| 72.0|    72.0|       -5.0|       1.0|              1.0|             -86.0|                0.0|               0.0| 5763.0|2013-01-02|\n",
            "|  217|    2|        2|   0|    0|  1|   0|                   24|          0|        0|         0|            2|                      13|              6|     2|    0|           0|            1|             1470.0|             4.0|              1.0|            -3.0|       100.0|         88.0|        75.0|              19.0|               13.0|       6.0| 56.0|    56.0|       -5.0|       1.0|              1.0|              -4.0|                0.0|               0.0| 6927.0|2013-01-02|\n",
            "|  275|    4|        2|   0|    0|  1|   0|                   24|         25|        3|         0|            2|                       8|              3|     0|    0|           0|            1|             6880.0|             6.0|              3.0|             0.0|       100.0|         85.0|        67.0|              19.0|               11.0|       6.0| 73.0|    73.0|       -5.0|       1.0|              1.0|             -86.0|                0.0|               0.0| 3444.0|2013-01-02|\n",
            "|  283|    6|        2|   0|    0|  1|   0|                   24|          0|        2|         0|            0|                      10|              0|    11|    0|           0|            1|              430.0|             7.0|              4.0|             1.0|        93.0|         85.0|        78.0|              24.0|               16.0|       6.0| 63.0|    63.0|       -5.0|       1.0|              1.0|             -86.0|                0.0|               0.0|11944.0|2013-01-02|\n",
            "|  286|    6|        2|   0|    0|  1|   0|                    0|         25|        3|         2|            1|                       0|              2|    11|    0|           0|            1|            22330.0|             7.0|              4.0|             1.0|        93.0|         85.0|        78.0|              24.0|               16.0|       6.0| 63.0|    63.0|       -5.0|       1.0|              1.0|             -86.0|                0.0|               0.0| 9661.0|2013-01-02|\n",
            "|  298|    2|        2|   0|    0|  1|   0|                    0|          0|        0|         2|            2|                      20|              6|     2|    0|           0|            1|             1140.0|             4.0|              1.0|            -3.0|       100.0|         88.0|        75.0|              19.0|               13.0|       6.0| 56.0|    56.0|       -5.0|       1.0|              1.0|              -4.0|                0.0|               0.0| 2961.0|2013-01-02|\n",
            "|  307|    9|        2|   0|    0|  1|   0|                    0|         25|        0|         0|            2|                       0|              3|    11|    0|           0|            1|              420.0|             5.0|              3.0|             2.0|        93.0|         78.0|        60.0|              29.0|               18.0|       6.0| 71.0|    71.0|       -5.0|       1.0|              1.0|             -86.0|                0.0|               0.0| 5791.0|2013-01-02|\n",
            "|  354|   11|        2|   0|    0|  1|   0|                    0|          0|        3|         2|            0|                       0|              0|    11|    0|           0|            1|            14160.0|             5.0|              3.0|             1.0|        93.0|         80.0|        67.0|              32.0|               19.0|       5.0| 65.0|    65.0|       -5.0|       1.0|              1.0|             -86.0|                0.0|               0.0| 5591.0|2013-01-02|\n",
            "|  364|   11|        2|   0|    0|  1|   0|                   24|         25|        0|         0|            2|                       8|              1|    11|    0|           0|            1|            17500.0|             5.0|              3.0|             1.0|        93.0|         80.0|        67.0|              32.0|               19.0|       5.0| 65.0|    65.0|       -5.0|       1.0|              1.0|             -86.0|                0.0|               0.0| 4116.0|2013-01-02|\n",
            "|  365|    7|        2|   0|    0|  1|   0|                    8|          0|        3|         2|            1|                      19|              5|     0|    0|           0|            1|             8400.0|             7.0|              3.0|            -1.0|       100.0|         85.0|        67.0|              18.0|               10.0|       6.0| 36.0|    36.0|       -5.0|       1.0|              1.0|             -86.0|                0.0|               0.0| 6425.0|2013-01-02|\n",
            "|  370|    6|        2|   0|    0|  1|   0|                   24|          0|        0|         0|            0|                       9|              0|    11|    0|           0|            1|             3130.0|             7.0|              4.0|             1.0|        93.0|         85.0|        78.0|              24.0|               16.0|       6.0| 63.0|    63.0|       -5.0|       1.0|              1.0|             -86.0|                0.0|               0.0| 4363.0|2013-01-02|\n",
            "|  403|    6|        2|   0|    0|  1|   0|                   24|          0|        0|         0|            0|                      10|              0|    11|    0|           0|            1|             2890.0|             7.0|              4.0|             1.0|        93.0|         85.0|        78.0|              24.0|               16.0|       6.0| 63.0|    63.0|       -5.0|       1.0|              1.0|             -86.0|                0.0|               0.0| 8887.0|2013-01-02|\n",
            "+-----+-----+---------+----+-----+---+----+---------------------+-----------+---------+----------+-------------+------------------------+---------------+------+-----+------------+-------------+-------------------+----------------+-----------------+----------------+------------+-------------+------------+------------------+-------------------+----------+-----+--------+-----------+----------+-----------------+------------------+-------------------+------------------+-------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "================\n",
            "Data frame sizes\n",
            "================\n",
            "Training: 806871\n",
            "Validation: 37467\n",
            "Test: 41088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVy5yTksV_7I",
        "colab_type": "code",
        "outputId": "6b6d90f9-8f6f-46ea-9dae-5530b0f91b68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Save data frames as Parquet files.\n",
        "train_df.write.parquet('train_df.parquet' , mode='overwrite')\n",
        "val_df.write.parquet('val_df.parquet', mode='overwrite')\n",
        "test_df.write.parquet('test_df.parquet' , mode='overwrite')\n",
        "\n",
        "spark.stop()\n",
        "\n",
        "\n",
        "print('==============')\n",
        "print('Model training')\n",
        "print('==============')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, Concatenate, Dense, Flatten, Reshape, BatchNormalization, Dropout\n",
        "import tensorflow.keras.backend as K\n",
        "import horovod.spark\n",
        "import horovod.tensorflow.keras as hvd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==============\n",
            "Model training\n",
            "==============\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aPNt6TKWHUq",
        "colab_type": "code",
        "outputId": "b521fa0e-0262-44fb-fbe4-c26564653ad7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3712
        }
      },
      "source": [
        "def exp_rmspe(y_true, y_pred):\n",
        "    \"\"\"Competition evaluation metric, expects logarithic inputs.\"\"\"\n",
        "    pct = tf.square((tf.exp(y_true) - tf.exp(y_pred)) / tf.exp(y_true))\n",
        "    # Compute mean excluding stores with zero denominator.\n",
        "    x = tf.reduce_sum(tf.where(y_true > 0.001, pct, tf.zeros_like(pct)))\n",
        "    y = tf.reduce_sum(tf.where(y_true > 0.001, tf.ones_like(pct), tf.zeros_like(pct)))\n",
        "    return tf.sqrt(x / y)\n",
        "\n",
        "\n",
        "def act_sigmoid_scaled(x):\n",
        "    \"\"\"Sigmoid scaled to logarithm of maximum sales scaled by 20%.\"\"\"\n",
        "    return tf.nn.sigmoid(x) * tf.log(max_sales) * 1.2\n",
        "\n",
        "\n",
        "CUSTOM_OBJECTS = {'exp_rmspe': exp_rmspe,\n",
        "                  'act_sigmoid_scaled': act_sigmoid_scaled}\n",
        "\n",
        "\n",
        "def serialize_model(model):\n",
        "    \"\"\"Serialize model into byte array.\"\"\"\n",
        "    bio = io.BytesIO()\n",
        "    with h5py.File(bio) as f:\n",
        "        model.save(f)\n",
        "    return bio.getvalue()\n",
        "\n",
        "\n",
        "def deserialize_model(model_bytes, load_model_fn):\n",
        "    \"\"\"Deserialize model from byte array.\"\"\"\n",
        "    bio = io.BytesIO(model_bytes)\n",
        "    with h5py.File(bio) as f:\n",
        "        return load_model_fn(f, custom_objects=CUSTOM_OBJECTS)\n",
        "\n",
        "\n",
        "# Do not use GPU for the session creation.\n",
        "config = tf.ConfigProto(device_count={'GPU': 0})\n",
        "K.set_session(tf.Session(config=config))\n",
        "\n",
        "# Build the model.\n",
        "inputs = {col: Input(shape=(1,), name=col) for col in all_cols}\n",
        "embeddings = [Embedding(len(vocab[col]), 10, input_length=1, name='emb_' + col)(inputs[col])\n",
        "              for col in categorical_cols]\n",
        "continuous_bn = Concatenate()([Reshape((1, 1), name='reshape_' + col)(inputs[col])\n",
        "                               for col in continuous_cols])\n",
        "continuous_bn = BatchNormalization()(continuous_bn)\n",
        "x = Concatenate()(embeddings + [continuous_bn])\n",
        "x = Flatten()(x)\n",
        "x = Dense(1000, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.00005))(x)\n",
        "x = Dense(1000, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.00005))(x)\n",
        "x = Dense(1000, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.00005))(x)\n",
        "x = Dense(500, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.00005))(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(1, activation=act_sigmoid_scaled)(x)\n",
        "model = tf.keras.Model([inputs[f] for f in all_cols], output)\n",
        "model.summary()\n",
        "\n",
        "# Horovod: add Distributed Optimizer.\n",
        "opt = tf.keras.optimizers.Adam(lr=LR, epsilon=1e-3)\n",
        "opt = hvd.DistributedOptimizer(opt)\n",
        "model.compile(opt, 'mae', metrics=[exp_rmspe])\n",
        "model_bytes = serialize_model(model)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "CompetitionDistance (InputLayer (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Max_TemperatureC (InputLayer)   (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Mean_TemperatureC (InputLayer)  (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Min_TemperatureC (InputLayer)   (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Max_Humidity (InputLayer)       (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Mean_Humidity (InputLayer)      (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Min_Humidity (InputLayer)       (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Max_Wind_SpeedKm_h (InputLayer) (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Mean_Wind_SpeedKm_h (InputLayer (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "CloudCover (InputLayer)         (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "trend (InputLayer)              (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "trend_DE (InputLayer)           (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "BeforePromo (InputLayer)        (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "AfterPromo (InputLayer)         (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "AfterStateHoliday (InputLayer)  (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "BeforeStateHoliday (InputLayer) (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "BeforeSchoolHoliday (InputLayer (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "AfterSchoolHoliday (InputLayer) (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "reshape_CompetitionDistance (Re (None, 1, 1)         0           CompetitionDistance[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "reshape_Max_TemperatureC (Resha (None, 1, 1)         0           Max_TemperatureC[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "reshape_Mean_TemperatureC (Resh (None, 1, 1)         0           Mean_TemperatureC[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "reshape_Min_TemperatureC (Resha (None, 1, 1)         0           Min_TemperatureC[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "reshape_Max_Humidity (Reshape)  (None, 1, 1)         0           Max_Humidity[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "reshape_Mean_Humidity (Reshape) (None, 1, 1)         0           Mean_Humidity[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape_Min_Humidity (Reshape)  (None, 1, 1)         0           Min_Humidity[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "reshape_Max_Wind_SpeedKm_h (Res (None, 1, 1)         0           Max_Wind_SpeedKm_h[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "reshape_Mean_Wind_SpeedKm_h (Re (None, 1, 1)         0           Mean_Wind_SpeedKm_h[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "reshape_CloudCover (Reshape)    (None, 1, 1)         0           CloudCover[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_trend (Reshape)         (None, 1, 1)         0           trend[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "reshape_trend_DE (Reshape)      (None, 1, 1)         0           trend_DE[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "reshape_BeforePromo (Reshape)   (None, 1, 1)         0           BeforePromo[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "reshape_AfterPromo (Reshape)    (None, 1, 1)         0           AfterPromo[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_AfterStateHoliday (Resh (None, 1, 1)         0           AfterStateHoliday[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "reshape_BeforeStateHoliday (Res (None, 1, 1)         0           BeforeStateHoliday[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "reshape_BeforeSchoolHoliday (Re (None, 1, 1)         0           BeforeSchoolHoliday[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "reshape_AfterSchoolHoliday (Res (None, 1, 1)         0           AfterSchoolHoliday[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Store (InputLayer)              (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "State (InputLayer)              (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "DayOfWeek (InputLayer)          (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Year (InputLayer)               (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Month (InputLayer)              (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Day (InputLayer)                (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Week (InputLayer)               (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "CompetitionMonthsOpen (InputLay (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Promo2Weeks (InputLayer)        (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "StoreType (InputLayer)          (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Assortment (InputLayer)         (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "PromoInterval (InputLayer)      (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "CompetitionOpenSinceYear (Input (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Promo2SinceYear (InputLayer)    (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Events (InputLayer)             (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Promo (InputLayer)              (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "StateHoliday (InputLayer)       (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "SchoolHoliday (InputLayer)      (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 1, 18)        0           reshape_CompetitionDistance[0][0]\n",
            "                                                                 reshape_Max_TemperatureC[0][0]   \n",
            "                                                                 reshape_Mean_TemperatureC[0][0]  \n",
            "                                                                 reshape_Min_TemperatureC[0][0]   \n",
            "                                                                 reshape_Max_Humidity[0][0]       \n",
            "                                                                 reshape_Mean_Humidity[0][0]      \n",
            "                                                                 reshape_Min_Humidity[0][0]       \n",
            "                                                                 reshape_Max_Wind_SpeedKm_h[0][0] \n",
            "                                                                 reshape_Mean_Wind_SpeedKm_h[0][0]\n",
            "                                                                 reshape_CloudCover[0][0]         \n",
            "                                                                 reshape_trend[0][0]              \n",
            "                                                                 reshape_trend_DE[0][0]           \n",
            "                                                                 reshape_BeforePromo[0][0]        \n",
            "                                                                 reshape_AfterPromo[0][0]         \n",
            "                                                                 reshape_AfterStateHoliday[0][0]  \n",
            "                                                                 reshape_BeforeStateHoliday[0][0] \n",
            "                                                                 reshape_BeforeSchoolHoliday[0][0]\n",
            "                                                                 reshape_AfterSchoolHoliday[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "emb_Store (Embedding)           (None, 1, 10)        11150       Store[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "emb_State (Embedding)           (None, 1, 10)        120         State[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "emb_DayOfWeek (Embedding)       (None, 1, 10)        70          DayOfWeek[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "emb_Year (Embedding)            (None, 1, 10)        30          Year[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "emb_Month (Embedding)           (None, 1, 10)        120         Month[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "emb_Day (Embedding)             (None, 1, 10)        310         Day[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "emb_Week (Embedding)            (None, 1, 10)        520         Week[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "emb_CompetitionMonthsOpen (Embe (None, 1, 10)        250         CompetitionMonthsOpen[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "emb_Promo2Weeks (Embedding)     (None, 1, 10)        260         Promo2Weeks[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "emb_StoreType (Embedding)       (None, 1, 10)        40          StoreType[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "emb_Assortment (Embedding)      (None, 1, 10)        30          Assortment[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "emb_PromoInterval (Embedding)   (None, 1, 10)        40          PromoInterval[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "emb_CompetitionOpenSinceYear (E (None, 1, 10)        230         CompetitionOpenSinceYear[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "emb_Promo2SinceYear (Embedding) (None, 1, 10)        80          Promo2SinceYear[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "emb_Events (Embedding)          (None, 1, 10)        220         Events[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "emb_Promo (Embedding)           (None, 1, 10)        20          Promo[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "emb_StateHoliday (Embedding)    (None, 1, 10)        20          StateHoliday[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "emb_SchoolHoliday (Embedding)   (None, 1, 10)        20          SchoolHoliday[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1 (BatchNo (None, 1, 18)        72          concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 1, 198)       0           emb_Store[0][0]                  \n",
            "                                                                 emb_State[0][0]                  \n",
            "                                                                 emb_DayOfWeek[0][0]              \n",
            "                                                                 emb_Year[0][0]                   \n",
            "                                                                 emb_Month[0][0]                  \n",
            "                                                                 emb_Day[0][0]                    \n",
            "                                                                 emb_Week[0][0]                   \n",
            "                                                                 emb_CompetitionMonthsOpen[0][0]  \n",
            "                                                                 emb_Promo2Weeks[0][0]            \n",
            "                                                                 emb_StoreType[0][0]              \n",
            "                                                                 emb_Assortment[0][0]             \n",
            "                                                                 emb_PromoInterval[0][0]          \n",
            "                                                                 emb_CompetitionOpenSinceYear[0][0\n",
            "                                                                 emb_Promo2SinceYear[0][0]        \n",
            "                                                                 emb_Events[0][0]                 \n",
            "                                                                 emb_Promo[0][0]                  \n",
            "                                                                 emb_StateHoliday[0][0]           \n",
            "                                                                 emb_SchoolHoliday[0][0]          \n",
            "                                                                 batch_normalization_v1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 198)          0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1000)         199000      flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1000)         1001000     dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1000)         1001000     dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 500)          500500      dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 500)          0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1)            501         dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,715,603\n",
            "Trainable params: 2,715,567\n",
            "Non-trainable params: 36\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDkKK5h_xPAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_fn(model_bytes):\n",
        "    # Make sure pyarrow is referenced before anything else to avoid segfault due to conflict\n",
        "    # with TensorFlow libraries.  Use `pa` package reference to ensure it's loaded before\n",
        "    # functions like `deserialize_model` which are implemented at the top level.\n",
        "    # See https://jira.apache.org/jira/browse/ARROW-3346\n",
        "    pa\n",
        "\n",
        "    import atexit\n",
        "    import horovod.tensorflow.keras as hvd\n",
        "    import os\n",
        "    from petastorm import make_batch_reader\n",
        "    from petastorm.tf_utils import make_petastorm_dataset\n",
        "    import tempfile\n",
        "    import tensorflow as tf\n",
        "    import tensorflow.keras.backend as K\n",
        "    import shutil\n",
        "\n",
        "    # Horovod: initialize Horovod inside the trainer.\n",
        "    hvd.init()\n",
        "\n",
        "    # Horovod: pin GPU to be used to process local rank (one GPU per process), if GPUs are available.\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
        "    K.set_session(tf.Session(config=config))\n",
        "\n",
        "    # Horovod: restore from checkpoint, use hvd.load_model under the hood.\n",
        "    model = deserialize_model(model_bytes, hvd.load_model)\n",
        "\n",
        "    # Horovod: adjust learning rate based on number of processes.\n",
        "    K.set_value(model.optimizer.lr, K.get_value(model.optimizer.lr) * hvd.size())\n",
        "\n",
        "    # Horovod: print summary logs on the first worker.\n",
        "    verbose = 2 if hvd.rank() == 0 else 0\n",
        "\n",
        "    callbacks = [\n",
        "        # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
        "        # This is necessary to ensure consistent initialization of all workers when\n",
        "        # training is started with random weights or restored from a checkpoint.\n",
        "        hvd.callbacks.BroadcastGlobalVariablesCallback(root_rank=0),\n",
        "\n",
        "        # Horovod: average metrics among workers at the end of every epoch.\n",
        "        #\n",
        "        # Note: This callback must be in the list before the ReduceLROnPlateau,\n",
        "        # TensorBoard, or other metrics-based callbacks.\n",
        "        hvd.callbacks.MetricAverageCallback(),\n",
        "\n",
        "        # Horovod: using `lr = 1.0 * hvd.size()` from the very beginning leads to worse final\n",
        "        # accuracy. Scale the learning rate `lr = 1.0` ---> `lr = 1.0 * hvd.size()` during\n",
        "        # the first five epochs. See https://arxiv.org/abs/1706.02677 for details.\n",
        "        hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=verbose),\n",
        "\n",
        "        # Reduce LR if the metric is not improved for 10 epochs, and stop training\n",
        "        # if it has not improved for 20 epochs.\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_exp_rmspe', patience=10, verbose=verbose),\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_exp_rmspe', mode='min', patience=20, verbose=verbose),\n",
        "        tf.keras.callbacks.TerminateOnNaN()\n",
        "    ]\n",
        "\n",
        "    # Model checkpoint location.\n",
        "    ckpt_dir = tempfile.mkdtemp()\n",
        "    ckpt_file = os.path.join(ckpt_dir, 'checkpoint.h5')\n",
        "    atexit.register(lambda: shutil.rmtree(ckpt_dir))\n",
        "\n",
        "    # Horovod: save checkpoints only on the first worker to prevent other workers from corrupting them.\n",
        "    if hvd.rank() == 0:\n",
        "        callbacks.append(tf.keras.callbacks.ModelCheckpoint(ckpt_file, monitor='val_exp_rmspe', mode='min',\n",
        "                                                            save_best_only=True))\n",
        "\n",
        "    # Make Petastorm readers.\n",
        "    with make_batch_reader('train_df.parquet' , num_epochs=None,\n",
        "                           cur_shard=hvd.rank(), shard_count=hvd.size(),\n",
        "                           hdfs_driver=PETASTORM_HDFS_DRIVER) as train_reader:\n",
        "        with make_batch_reader('val_df.parquet' , num_epochs=None,\n",
        "                               cur_shard=hvd.rank(), shard_count=hvd.size(),\n",
        "                               hdfs_driver=PETASTORM_HDFS_DRIVER) as val_reader:\n",
        "            # Convert readers to tf.data.Dataset.\n",
        "            train_ds = make_petastorm_dataset(train_reader) \\\n",
        "                .apply(tf.data.experimental.unbatch()) \\\n",
        "                .shuffle(int(train_rows / hvd.size())) \\\n",
        "                .batch(BATCH_SIZE) \\\n",
        "                .map(lambda x: (tuple(getattr(x, col) for col in all_cols), tf.log(x.Sales)))\n",
        "\n",
        "            val_ds = make_petastorm_dataset(val_reader) \\\n",
        "                .apply(tf.data.experimental.unbatch()) \\\n",
        "                .batch(BATCH_SIZE) \\\n",
        "                .map(lambda x: (tuple(getattr(x, col) for col in all_cols), tf.log(x.Sales)))\n",
        "\n",
        "            history = model.fit(train_ds,\n",
        "                                validation_data=val_ds,\n",
        "                                steps_per_epoch=int(train_rows / BATCH_SIZE / hvd.size()),\n",
        "                                validation_steps=int(val_rows / BATCH_SIZE / hvd.size()),\n",
        "                                callbacks=callbacks,\n",
        "                                verbose=verbose,\n",
        "                                epochs=1)\n",
        "\n",
        "    # Dataset API usage currently displays a wall of errors upon termination.\n",
        "    # This global model registration ensures clean termination.\n",
        "    # Tracked in https://github.com/tensorflow/tensorflow/issues/24570\n",
        "    globals()['_DATASET_FINALIZATION_HACK'] = model\n",
        "\n",
        "    if hvd.rank() == 0:\n",
        "        with open(ckpt_file, 'rb') as f:\n",
        "            return history.history, f.read()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNwaXJVOyHmM",
        "colab_type": "code",
        "outputId": "ac691e50-838b-44bb-e0ac-bd67b38b26dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1319
        }
      },
      "source": [
        "# Create Spark session for training.\n",
        "conf = SparkConf().setAppName('data_prep').set('spark.sql.shuffle.partitions', '64')\n",
        "\n",
        "conf = SparkConf().setAppName('training').set('spark.sql.shuffle.partitions', '64')\n",
        "if TRAINING_CLUSTER:\n",
        "    conf.setMaster(TRAINING_CLUSTER)\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
        "\n",
        "# Horovod: run training.\n",
        "history, best_model_bytes = \\\n",
        "    horovod.spark.run(train_fn, args=(model_bytes,), num_proc=NUM_TRAINING_PROC, verbose=2)[0]\n",
        "\n",
        "best_val_rmspe = min(history['val_exp_rmspe'])\n",
        "print('Best RMSPE: %f' % best_val_rmspe)\n",
        "\n",
        "# Write checkpoint.\n",
        "with open(LOCAL_CHECKPOINT_FILE, 'wb') as f:\n",
        "    f.write(best_model_bytes)\n",
        "print('Written checkpoint to %s' % LOCAL_CHECKPOINT_FILE)\n",
        "\n",
        "spark.stop()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running 4 processes...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-40:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/horovod/spark/__init__.py\", line 81, in run_spark\n",
            "    result = procs.mapPartitionsWithIndex(_make_mapper(driver.addresses(), settings)).collect()\n",
            "  File \"/content/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 816, in collect\n",
            "    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
            "  File \"/content/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
            "    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/content/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\", line 63, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/content/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
            "    format(target_id, \".\", name), value)\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
            ": org.apache.spark.SparkException: Job 0 cancelled part of cancelled job group horovod.spark.run.1\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1824)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:906)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:906)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:906)\n",
            "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:906)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2079)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
            "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
            "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n",
            "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n",
            "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
            "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
            "\tat java.lang.Thread.run(Thread.java:748)\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-de561b9b6089>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Horovod: run training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model_bytes\u001b[0m \u001b[0;34m=\u001b[0m     \u001b[0mhorovod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_bytes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_proc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_TRAINING_PROC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mbest_val_rmspe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_exp_rmspe'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/horovod/spark/__init__.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(fn, args, kwargs, num_proc, start_timeout, env, stdout, stderr, verbose)\u001b[0m\n\u001b[1;32m    148\u001b[0m                                       result_queue, settings)\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_initial_registration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Initial Spark task registration is complete.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/horovod/spark/driver/driver_service.py\u001b[0m in \u001b[0;36mwait_for_initial_registration\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_for_spark_job_failure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_time_out_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Spark tasks to start'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/horovod/run/common/util/timeout.py\u001b[0m in \u001b[0;36mcheck_time_out_for\u001b[0;34m(self, activity)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_time_out_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimed_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactivity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: Timed out waiting for Spark tasks to start. Please check that you have enough resources to run all Horovod processes. Each Horovod process runs in a Spark task. You may need to increase the start_timeout parameter to a larger value if your Spark resources are allocated on-demand."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqE9yN87yKt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_bytes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI2my8xFyV51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install h5py >=2.9.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3aFf3IX0KfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxsXeDOH0kln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h5py.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwWCeg8o0mcs",
        "colab_type": "code",
        "outputId": "0023e62b-5496-4794-a28e-f9340e996339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!pip uninstall h5py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling h5py-2.9.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/h5py-2.9.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/h5py/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled h5py-2.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGC0rjsE0t8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install h5py>=2.9.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THZv4jWU0zQe",
        "colab_type": "code",
        "outputId": "43eb4921-0a7c-4278-e466-1566886dc41a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "import importlib\n",
        "importlib.reload(h5py)\n",
        "h5py.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-28a30e475d28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5py\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/__init__.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__spec__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkgpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;31m# The module may have replaced itself in sys.modules!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_exec\u001b[0;34m(spec, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mh5t\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpy_get_enum\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mget_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mh5py_warnings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModuleWrapper\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_ModuleWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0mhighlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ModuleWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"h5py.highlevel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'ModuleWrapper'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G224MeCu01-7",
        "colab_type": "code",
        "outputId": "06456f8e-a6c9-48ef-fa29-d201af186ca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "model_bytes = serialize_model(model)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-caa8161f1119>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-77-0f94d16c36f1>\u001b[0m in \u001b[0;36mserialize_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;34m\"\"\"Serialize model into byte array.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mbio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbio\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    307\u001b[0m     def __init__(self, name, mode=None, driver=None,\n\u001b[1;32m    308\u001b[0m                  \u001b[0mlibver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                  \u001b[0mrdcc_nslots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_nbytes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_w0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m                  \u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                  **kwds):\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/compat.py\u001b[0m in \u001b[0;36mfilename_encode\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mfilenames\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0minformation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \"\"\"\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"win32\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not _io.BytesIO"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTdh7VXc1Fir",
        "colab_type": "code",
        "outputId": "24a7643d-1854-4a0d-ac92-c77b4e107022",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "h5py.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oeEQx0bLTLB",
        "colab_type": "code",
        "outputId": "d0cac8d7-20b1-4cfd-d401-36effbaf81c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!pip uninstall h5py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling h5py-2.8.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/h5py-2.8.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/h5py/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled h5py-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR0HhgJjLVsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install h5py>=2.9.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obnPYzjWLavr",
        "colab_type": "code",
        "outputId": "d99d0e66-7733-487a-fb14-e9efa1efbfa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "import importlib\n",
        "importlib.reload(h5py)\n",
        "h5py.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-28a30e475d28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5py\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/__init__.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__spec__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkgpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;31m# The module may have replaced itself in sys.modules!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_exec\u001b[0;34m(spec, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mh5t\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpy_get_enum\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mget_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mh5py_warnings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModuleWrapper\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_ModuleWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0mhighlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ModuleWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"h5py.highlevel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'ModuleWrapper'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRY68J2HLgSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}