{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of pruning_with_keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "JCkPHee-alKM"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jimmymanianchira/RandomExperiments/blob/master/Copy_of_pruning_with_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JCkPHee-alKM"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "cvrknnPsaqaM",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Zt0JEl38SWX"
      },
      "source": [
        "# Magnitude-based weight pruning with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IwBSTsryazfT"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/g3doc/guide/pruning/pruning_with_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/g3doc/guide/pruning/pruning_with_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L7IGLmaw5DjA"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Welcome to the tutorial for *weight pruning*, part of the TensorFlow Model Optimization toolkit.\n",
        "\n",
        "#### What is weight pruning?\n",
        "\n",
        "Weight pruning means literally that: eliminating unnecessary values in the weight tensor. We are practically setting neural network parameters' values to zero to remove low-weight connections between the *layers* of a neural network.\n",
        "\n",
        "#### Why is useful?\n",
        "\n",
        "Tensors with several values set to zero can be considered *sparse*. This results in important benefits:\n",
        "* *Compression*. Sparse tensors are amenable to compression by only keeping the non-zero values and their corresponding coordinates.\n",
        "* *Speed*. Sparse tensors allow us to skip otherwise unnecessary computations involving the zero values.\n",
        "\n",
        "#### How does it work?\n",
        "\n",
        "Our Keras-based weight pruning API is designed to iteratively remove connections based on their magnitude, during training. For more details on the usage of the API, please refer to the GitHub page.\n",
        "\n",
        "In this tutorial, we'll walk you through an end-to-end example of using the weight pruning API on a simple MNIST model. We will show that by simply using a generic file compression algorithm (e.g. zip) the Keras model will be reduced in size, and that this size reduction persists when converted to a Tensorflow Lite format.\n",
        "\n",
        "Two things worth clarifying:\n",
        "* The technique and API are not TensorFlow Lite specific --we just show its application on the TensorFlow Lite backend, as it covers size-sensitive use-cases.\n",
        "* By itself, a sparse model will not be faster to execute. It just enables backends with such capability. In the near future, however, TensorFlow Lite will take advantage of the sparsity to speed up computations.\n",
        "\n",
        "To recap, in the tutorial we will:\n",
        "1.   Train a MNIST model with Keras from scratch.\n",
        "2.   Train a pruned MNIST with the pruning API.\n",
        "3.   Compare the size of the pruned model and the non-pruned one after compression.\n",
        "4.   Convert the pruned model to Tensorflow Lite format and verify that accuracy persists.\n",
        "5.   Show how the pruned model works with other optimization techniques, like post-training quantization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P8qFbkru8FKu"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q8zIQsT9mUTw"
      },
      "source": [
        "To use the pruning API, you need to install the `tensorflow-model-optimization` and `tf-nightly` packages.\n",
        "\n",
        "Since you will train a few models in this tutorial, install the `tensorflow-gpu` package to speed up things. Enable the GPU with: *Runtime > Change runtime type > Hardware accelator* and make sure GPU is selected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pn836LSTNSHA",
        "outputId": "313ea445-2693-4331-aa71-341b7715da92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "source": [
        "! pip uninstall -y tensorflow\n",
        "! pip uninstall -y tf-nightly\n",
        "! pip install -U tf-nightly-gpu\n",
        "\n",
        "! pip install tensorflow-model-optimization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n",
            "\u001b[33mWARNING: Skipping tf-nightly as it is not installed.\u001b[0m\n",
            "Collecting tf-nightly-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/8d/bbc892278fa13242fa8c6de1c4fc5b84e9e8a184ae859ee1dcadf333c0a7/tf_nightly_gpu-1.14.1.dev20190604-cp36-cp36m-manylinux1_x86_64.whl (378.0MB)\n",
            "\u001b[K     |████████████████████████████████| 378.0MB 52kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.16.4)\n",
            "Collecting wrapt>=1.11.1 (from tf-nightly-gpu)\n",
            "  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n",
            "Collecting tb-nightly<1.15.0a0,>=1.14.0a0 (from tf-nightly-gpu)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/9b/2d21e3ae6a04c3db24720c918f1e5d36c31f1a3f0a3e100138bd960f6064/tb_nightly-1.14.0a20190604-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 31.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.0.9)\n",
            "Collecting google-pasta>=0.1.6 (from tf-nightly-gpu)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 29.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.7.1)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.33.4)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.0.7)\n",
            "Collecting tf-estimator-nightly (from tf-nightly-gpu)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/f7/013d6b53031c427774b5c31ea745de6657f959ba7c63f6228414fff429a9/tf_estimator_nightly-1.14.0.dev2019060401-py2.py3-none-any.whl (490kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 52.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu) (0.15.4)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu) (41.0.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-gpu) (2.8.0)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n",
            "Successfully built wrapt\n",
            "\u001b[31mERROR: thinc 6.12.1 has requirement wrapt<1.11.0,>=1.10.0, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: wrapt, tb-nightly, google-pasta, tf-estimator-nightly, tf-nightly-gpu\n",
            "  Found existing installation: wrapt 1.10.11\n",
            "    Uninstalling wrapt-1.10.11:\n",
            "      Successfully uninstalled wrapt-1.10.11\n",
            "Successfully installed google-pasta-0.1.7 tb-nightly-1.14.0a20190604 tf-estimator-nightly-1.14.0.dev2019060401 tf-nightly-gpu-1.14.1.dev20190604 wrapt-1.11.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-model-optimization in /usr/local/lib/python3.6/dist-packages (0.1.1)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.16.4)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1ykjgo4UNXmD",
        "outputId": "99b720a8-78f1-42bd-b38d-38291e9f4383",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%load_ext tensorboard\n",
        "import tensorboard"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard module is not an IPython extension.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhiTzeWTh1De",
        "colab_type": "code",
        "outputId": "e5a65e6a-9490-4882-d36f-8d6cfff5579f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6413
        }
      },
      "source": [
        "!pip freeze"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "absl-py==0.7.1\n",
            "alabaster==0.7.12\n",
            "albumentations==0.1.12\n",
            "altair==3.0.1\n",
            "astor==0.8.0\n",
            "astropy==3.0.5\n",
            "atari-py==0.1.15\n",
            "atomicwrites==1.3.0\n",
            "attrs==19.1.0\n",
            "audioread==2.1.7\n",
            "autograd==1.2\n",
            "Babel==2.7.0\n",
            "backcall==0.1.0\n",
            "backports.tempfile==1.0\n",
            "backports.weakref==1.0.post1\n",
            "beautifulsoup4==4.6.3\n",
            "bleach==3.1.0\n",
            "bokeh==1.0.4\n",
            "boto==2.49.0\n",
            "boto3==1.9.157\n",
            "botocore==1.12.157\n",
            "Bottleneck==1.2.1\n",
            "branca==0.3.1\n",
            "bs4==0.0.1\n",
            "bz2file==0.98\n",
            "cachetools==3.1.1\n",
            "certifi==2019.3.9\n",
            "cffi==1.12.3\n",
            "chainer==5.4.0\n",
            "chardet==3.0.4\n",
            "Click==7.0\n",
            "cloudpickle==0.6.1\n",
            "cmake==3.12.0\n",
            "colorlover==0.3.0\n",
            "community==1.0.0b1\n",
            "contextlib2==0.5.5\n",
            "convertdate==2.1.3\n",
            "coverage==3.7.1\n",
            "coveralls==0.5\n",
            "crcmod==1.7\n",
            "cufflinks==0.14.6\n",
            "cupy-cuda100==5.4.0\n",
            "cvxopt==1.2.3\n",
            "cvxpy==1.0.15\n",
            "cycler==0.10.0\n",
            "cymem==2.0.2\n",
            "Cython==0.29.9\n",
            "cytoolz==0.9.0.1\n",
            "daft==0.0.4\n",
            "dask==1.1.5\n",
            "dataclasses==0.6\n",
            "datascience==0.10.6\n",
            "decorator==4.4.0\n",
            "defusedxml==0.6.0\n",
            "dill==0.2.9\n",
            "distributed==1.25.3\n",
            "Django==2.2.1\n",
            "dlib==19.16.0\n",
            "dm-sonnet==1.32\n",
            "docopt==0.6.2\n",
            "docutils==0.14\n",
            "dopamine-rl==1.0.5\n",
            "easydict==1.9\n",
            "ecos==2.0.7.post1\n",
            "editdistance==0.5.3\n",
            "en-core-web-sm==2.0.0\n",
            "entrypoints==0.3\n",
            "enum34==1.1.6\n",
            "ephem==3.7.6.0\n",
            "et-xmlfile==1.0.1\n",
            "fa2==0.3.5\n",
            "fancyimpute==0.4.3\n",
            "fastai==1.0.52\n",
            "fastcache==1.1.0\n",
            "fastdtw==0.3.2\n",
            "fastprogress==0.1.21\n",
            "fastrlock==0.4\n",
            "fbprophet==0.5\n",
            "feather-format==0.4.0\n",
            "featuretools==0.4.1\n",
            "filelock==3.0.12\n",
            "fix-yahoo-finance==0.0.22\n",
            "Flask==1.0.3\n",
            "folium==0.8.3\n",
            "future==0.16.0\n",
            "gast==0.2.2\n",
            "GDAL==2.2.2\n",
            "gdown==3.6.4\n",
            "gensim==3.6.0\n",
            "geographiclib==1.49\n",
            "geopy==1.17.0\n",
            "gevent==1.4.0\n",
            "gin-config==0.1.4\n",
            "glob2==0.6\n",
            "google==2.0.2\n",
            "google-api-core==1.11.1\n",
            "google-api-python-client==1.6.7\n",
            "google-auth==1.4.2\n",
            "google-auth-httplib2==0.0.3\n",
            "google-auth-oauthlib==0.3.0\n",
            "google-cloud-bigquery==1.8.1\n",
            "google-cloud-core==0.29.1\n",
            "google-cloud-language==1.0.2\n",
            "google-cloud-storage==1.13.2\n",
            "google-cloud-translate==1.3.3\n",
            "google-colab==1.0.0\n",
            "google-pasta==0.1.7\n",
            "google-resumable-media==0.3.2\n",
            "googleapis-common-protos==1.6.0\n",
            "googledrivedownloader==0.4\n",
            "graph-nets==1.0.4\n",
            "graphviz==0.10.1\n",
            "greenlet==0.4.15\n",
            "grpcio==1.15.0\n",
            "gspread==3.0.1\n",
            "gspread-dataframe==3.0.2\n",
            "gunicorn==19.9.0\n",
            "gym==0.10.11\n",
            "h5py==2.8.0\n",
            "HeapDict==1.0.0\n",
            "holidays==0.9.10\n",
            "html5lib==1.0.1\n",
            "httpimport==0.5.16\n",
            "httplib2==0.11.3\n",
            "humanize==0.5.1\n",
            "hyperopt==0.1.2\n",
            "ideep4py==2.0.0.post3\n",
            "idna==2.8\n",
            "image==1.5.27\n",
            "imageio==2.4.1\n",
            "imagesize==1.1.0\n",
            "imbalanced-learn==0.4.3\n",
            "imblearn==0.0\n",
            "imgaug==0.2.9\n",
            "importlib-metadata==0.17\n",
            "imutils==0.5.2\n",
            "inflect==2.1.0\n",
            "intel-openmp==2019.0\n",
            "intervaltree==2.1.0\n",
            "ipykernel==4.6.1\n",
            "ipython==5.5.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.3.9\n",
            "ipywidgets==7.4.2\n",
            "itsdangerous==1.1.0\n",
            "jdcal==1.4.1\n",
            "jedi==0.13.3\n",
            "jieba==0.39\n",
            "Jinja2==2.10.1\n",
            "jmespath==0.9.4\n",
            "joblib==0.13.2\n",
            "jpeg4py==0.1.4\n",
            "jsonschema==2.6.0\n",
            "jupyter==1.0.0\n",
            "jupyter-client==5.2.4\n",
            "jupyter-console==6.0.0\n",
            "jupyter-core==4.4.0\n",
            "kaggle==1.5.4\n",
            "kapre==0.1.3.1\n",
            "Keras==2.2.4\n",
            "Keras-Applications==1.0.7\n",
            "Keras-Preprocessing==1.0.9\n",
            "keras-vis==0.4.1\n",
            "kiwisolver==1.1.0\n",
            "knnimpute==0.1.0\n",
            "librosa==0.6.3\n",
            "lightgbm==2.2.3\n",
            "llvmlite==0.28.0\n",
            "lmdb==0.94\n",
            "lucid==0.3.8\n",
            "lunardate==0.2.0\n",
            "lxml==4.2.6\n",
            "magenta==0.3.19\n",
            "Markdown==3.1.1\n",
            "MarkupSafe==1.1.1\n",
            "matplotlib==3.0.3\n",
            "matplotlib-venn==0.11.5\n",
            "mesh-tensorflow==0.0.5\n",
            "mido==1.2.6\n",
            "mir-eval==0.5\n",
            "missingno==0.4.1\n",
            "mistune==0.8.4\n",
            "mkl==2019.0\n",
            "mlxtend==0.14.0\n",
            "mock==3.0.5\n",
            "more-itertools==7.0.0\n",
            "moviepy==0.2.3.5\n",
            "mpi4py==3.0.1\n",
            "mpmath==1.1.0\n",
            "msgpack==0.5.6\n",
            "msgpack-numpy==0.4.3.2\n",
            "multiprocess==0.70.7\n",
            "multitasking==0.0.9\n",
            "murmurhash==1.0.2\n",
            "music21==5.5.0\n",
            "natsort==5.5.0\n",
            "nbconvert==5.5.0\n",
            "nbformat==4.4.0\n",
            "networkx==2.3\n",
            "nibabel==2.3.3\n",
            "nltk==3.2.5\n",
            "nose==1.3.7\n",
            "notebook==5.2.2\n",
            "np-utils==0.5.10.0\n",
            "numba==0.40.1\n",
            "numexpr==2.6.9\n",
            "numpy==1.16.4\n",
            "nvidia-ml-py3==7.352.0\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.0.1\n",
            "okgrade==0.4.3\n",
            "olefile==0.46\n",
            "opencv-contrib-python==3.4.3.18\n",
            "opencv-python==3.4.5.20\n",
            "openpyxl==2.5.9\n",
            "osqp==0.5.0\n",
            "packaging==19.0\n",
            "pandas==0.24.2\n",
            "pandas-datareader==0.7.0\n",
            "pandas-gbq==0.4.1\n",
            "pandas-profiling==1.4.1\n",
            "pandocfilters==1.4.2\n",
            "parso==0.4.0\n",
            "pathlib==1.0.1\n",
            "patsy==0.5.1\n",
            "pexpect==4.7.0\n",
            "pickleshare==0.7.5\n",
            "Pillow==4.3.0\n",
            "pip-tools==3.6.1\n",
            "plac==0.9.6\n",
            "plotly==3.6.1\n",
            "pluggy==0.7.1\n",
            "portpicker==1.2.0\n",
            "prefetch-generator==1.0.1\n",
            "preshed==2.0.1\n",
            "pretty-midi==0.2.8\n",
            "prettytable==0.7.2\n",
            "progressbar2==3.38.0\n",
            "prometheus-client==0.6.0\n",
            "promise==2.2.1\n",
            "prompt-toolkit==1.0.16\n",
            "protobuf==3.7.1\n",
            "psutil==5.4.8\n",
            "psycopg2==2.7.6.1\n",
            "ptyprocess==0.6.0\n",
            "py==1.8.0\n",
            "pyarrow==0.13.0\n",
            "pyasn1==0.4.5\n",
            "pyasn1-modules==0.2.5\n",
            "pycocotools==2.0.0\n",
            "pycparser==2.19\n",
            "pydot==1.3.0\n",
            "pydot-ng==2.0.0\n",
            "pydotplus==2.0.2\n",
            "pyemd==0.5.1\n",
            "pyglet==1.3.2\n",
            "Pygments==2.1.3\n",
            "pygobject==3.26.1\n",
            "pymc3==3.7\n",
            "pymongo==3.8.0\n",
            "pymystem3==0.2.0\n",
            "PyOpenGL==3.1.0\n",
            "pyparsing==2.4.0\n",
            "pyrsistent==0.15.2\n",
            "pysndfile==1.3.2\n",
            "PySocks==1.7.0\n",
            "pystan==2.19.0.0\n",
            "pytest==3.6.4\n",
            "python-apt==1.6.4\n",
            "python-chess==0.23.11\n",
            "python-dateutil==2.5.3\n",
            "python-louvain==0.13\n",
            "python-rtmidi==1.3.0\n",
            "python-slugify==3.0.2\n",
            "python-utils==2.3.0\n",
            "pytz==2018.9\n",
            "PyWavelets==1.0.3\n",
            "PyYAML==3.13\n",
            "pyzmq==17.0.0\n",
            "qtconsole==4.5.1\n",
            "regex==2018.1.10\n",
            "requests==2.21.0\n",
            "requests-oauthlib==1.2.0\n",
            "resampy==0.2.1\n",
            "retrying==1.3.3\n",
            "rpy2==2.9.5\n",
            "rsa==4.0\n",
            "s3fs==0.2.1\n",
            "s3transfer==0.2.0\n",
            "scikit-image==0.15.0\n",
            "scikit-learn==0.21.2\n",
            "scipy==1.3.0\n",
            "screen-resolution-extra==0.0.0\n",
            "scs==2.1.0\n",
            "seaborn==0.9.0\n",
            "semantic-version==2.6.0\n",
            "Send2Trash==1.5.0\n",
            "setuptools-git==1.2\n",
            "Shapely==1.6.4.post2\n",
            "simplegeneric==0.8.1\n",
            "six==1.12.0\n",
            "sklearn==0.0\n",
            "sklearn-pandas==1.8.0\n",
            "smart-open==1.8.3\n",
            "snowballstemmer==1.2.1\n",
            "sortedcontainers==2.1.0\n",
            "spacy==2.0.18\n",
            "Sphinx==1.8.5\n",
            "sphinxcontrib-websupport==1.1.2\n",
            "SQLAlchemy==1.3.4\n",
            "sqlparse==0.3.0\n",
            "stable-baselines==2.2.1\n",
            "statsmodels==0.9.0\n",
            "sympy==1.1.1\n",
            "tables==3.4.4\n",
            "tabulate==0.8.3\n",
            "tb-nightly==1.14.0a20190604\n",
            "tblib==1.4.0\n",
            "tensor2tensor==1.11.0\n",
            "tensorboard==1.13.1\n",
            "tensorboardcolab==0.0.22\n",
            "tensorflow-estimator==1.13.0\n",
            "tensorflow-hub==0.4.0\n",
            "tensorflow-metadata==0.13.0\n",
            "tensorflow-model-optimization==0.1.1\n",
            "tensorflow-probability==0.6.0\n",
            "termcolor==1.1.0\n",
            "terminado==0.8.2\n",
            "testpath==0.4.2\n",
            "text-unidecode==1.2\n",
            "textblob==0.15.3\n",
            "textgenrnn==1.4.1\n",
            "tf-estimator-nightly==1.14.0.dev2019060401\n",
            "tf-nightly-gpu==1.14.1.dev20190604\n",
            "tfds-nightly==1.0.2.dev201905290106\n",
            "tflearn==0.3.2\n",
            "Theano==1.0.4\n",
            "thinc==6.12.1\n",
            "toolz==0.9.0\n",
            "torch==1.1.0\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.3.1\n",
            "torchvision==0.3.0\n",
            "tornado==4.5.3\n",
            "tqdm==4.28.1\n",
            "traitlets==4.3.2\n",
            "tweepy==3.6.0\n",
            "typing==3.6.6\n",
            "tzlocal==1.5.1\n",
            "ujson==1.35\n",
            "umap-learn==0.3.9\n",
            "uritemplate==3.0.0\n",
            "urllib3==1.24.3\n",
            "vega-datasets==0.7.0\n",
            "wcwidth==0.1.7\n",
            "webencodings==0.5.1\n",
            "Werkzeug==0.15.4\n",
            "widgetsnbextension==3.4.2\n",
            "wordcloud==1.5.0\n",
            "wrapt==1.11.1\n",
            "xarray==0.11.3\n",
            "xgboost==0.90\n",
            "xkit==0.0.0\n",
            "xlrd==1.1.0\n",
            "xlwt==1.3.0\n",
            "yellowbrick==0.9.1\n",
            "zict==0.1.4\n",
            "zipp==0.5.1\n",
            "zmq==0.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mydXeQlDNbnR",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gBYltugp-MdR"
      },
      "source": [
        "## Prepare the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P_MJqxz5z2dh",
        "outputId": "489994f4-ad28-4978-89c8-688677d251fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 10\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "if tf.keras.backend.image_data_format() == 'channels_first':\n",
        "  x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "  x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "  input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "  x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "  x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "  input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OmdnYKPK--5L"
      },
      "source": [
        "## Train a MNIST model without pruning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r1p_y3gPWeW2"
      },
      "source": [
        "### Build the MNIST model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uLg0SGdYp2Q6",
        "outputId": "761687b1-9031-42a4-dc90-e3e3f0927970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "l = tf.keras.layers\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    l.Conv2D(\n",
        "        32, 5, padding='same', activation='relu', input_shape=input_shape),\n",
        "    l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
        "    l.BatchNormalization(),\n",
        "    l.Conv2D(64, 5, padding='same', activation='relu'),\n",
        "    l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
        "    l.Flatten(),\n",
        "    l.Dense(1024, activation='relu'),\n",
        "    l.Dropout(0.4),\n",
        "    l.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 32)        832       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 14, 14, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              3212288   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 3,274,762\n",
            "Trainable params: 3,274,698\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5dHynoso_xXF"
      },
      "source": [
        "### Train the model to reach an accuracy >99%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KGo0UICmA8J3"
      },
      "source": [
        "\n",
        "Load [TensorBoard](https://www.tensorflow.org/tensorboard) to monitor the training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WJyS_IcQBBqg",
        "outputId": "619336c5-671e-425e-ca47-aa006eecad16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "logdir = tempfile.mkdtemp()\n",
        "print('Writing training logs to ' + logdir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing training logs to /tmp/tmpto9gfnex\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QfgjuOLj_mFu",
        "outputId": "da2df45a-1b6d-43d2-97aa-4c0ae9407f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorboard --logdir={logdir}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%tensorboard` not found.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A7c-fsQpTzOr",
        "outputId": "1aaed5a4-2a99-4f28-eaff-e02797c87ac3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        }
      },
      "source": [
        "callbacks = [tf.keras.callbacks.TensorBoard(log_dir=logdir, profile_batch=0)]\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          callbacks=callbacks,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0604 17:57:05.948760 140389961176960 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 8s 134us/sample - loss: 0.1855 - acc: 0.9489 - val_loss: 0.1501 - val_acc: 0.9858\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0440 - acc: 0.9861 - val_loss: 0.0358 - val_acc: 0.9878\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0323 - acc: 0.9898 - val_loss: 0.0252 - val_acc: 0.9914\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0249 - acc: 0.9923 - val_loss: 0.0279 - val_acc: 0.9907\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0206 - acc: 0.9936 - val_loss: 0.0283 - val_acc: 0.9907\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0191 - acc: 0.9940 - val_loss: 0.0358 - val_acc: 0.9886\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0155 - acc: 0.9953 - val_loss: 0.0305 - val_acc: 0.9909\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0146 - acc: 0.9953 - val_loss: 0.0264 - val_acc: 0.9928\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0122 - acc: 0.9962 - val_loss: 0.0286 - val_acc: 0.9921\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0104 - acc: 0.9966 - val_loss: 0.0316 - val_acc: 0.9915\n",
            "Test loss: 0.031643318830800286\n",
            "Test accuracy: 0.9915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUFqewQ9s160",
        "colab_type": "code",
        "outputId": "d31aecf0-d911-45f0-ca52-94b767716360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%timeit model.evaluate(x_train, y_train, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 3.62 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eLgMavDVCde5"
      },
      "source": [
        "### Save the original model for size comparison later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8E6U7GUIx5r9",
        "outputId": "5771c4fa-b09d-4eb1-a7f5-2af69cc240c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Backend agnostic way to save/restore models\n",
        "_, keras_file = tempfile.mkstemp('.h5')\n",
        "print('Saving model to: ', keras_file)\n",
        "tf.keras.models.save_model(model, keras_file, include_optimizer=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to:  /tmp/tmpdddbmkt6.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gWiQbobKC0NZ"
      },
      "source": [
        "## Train a pruned MNIST\n",
        "\n",
        "We provide a `prune_low_magnitude()` API to train models with removed connections. The Keras-based API can be applied at the level of individual layers, or the entire model. We will show you the usage of both in the following sections.\n",
        "\n",
        "At a high level, the technique works by iteratively removing (i.e. zeroing out) connections between layers, given an schedule and a target sparsity.\n",
        "\n",
        "For example, a typical configuration will target a 75% sparsity, by pruning connections every 100 steps (aka epochs), starting from step 2,000. For more details on the possible configurations, please refer to the github documentation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FfJ6Tm3KXMFY"
      },
      "source": [
        "### Build a pruned model layer by layer\n",
        "In this example, we show how to use the API at the level of layers, and build a pruned MNIST solver model.\n",
        "\n",
        "In this case, the `prune_low_magnitude(`) \n",
        "receives as parameter the Keras layer whose weights we want pruned.\n",
        "\n",
        "This function requires a pruning params which configures the pruning algorithm during training. Please refer to our github page for detailed documentation. The parameter used here means:\n",
        "\n",
        "\n",
        "1.   **Sparsity.** PolynomialDecay is used across the whole training process. We start at the sparsity level 50% and gradually train the model to reach 90% sparsity. X% sparsity means that X% of the weight tensor is going to be pruned away.\n",
        "2.   **Schedule**. Connections are pruned starting from step 2000 to the end of training, and runs every 100 steps. The reasoning behind this is that we want to train the model without pruning for a few epochs to reach a certain accuracy, to aid convergence. Furthermore, we give the model some time to recover after each pruning step, so pruning does not happen on every step. We set the pruning frequency to 100.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ip8qCtSlU3TQ",
        "colab": {}
      },
      "source": [
        "from tensorflow_model_optimization.sparsity import keras as sparsity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tA9rnRUrebTE"
      },
      "source": [
        "To demonstrate how to save and restore a pruned keras model, in the following example we first train the model for 10 epochs, save it to disk, and finally restore and continue training for 2 epochs. With gradual sparsity, four important parameters are begin_sparsity, final_sparsity, begin_step and end_step. The first three are straight forward. Let's calculate the end step given the number of train example, batch size, and the total epochs to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rrs-xoB2cSSQ",
        "outputId": "01f99054-f971-4c5e-e76a-d780fbe95b4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "epochs = 12\n",
        "num_train_samples = x_train.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print('End step: ' + str(end_step))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "End step: 5628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Shz-r2RiqFca",
        "outputId": "617b6912-40a9-46f6-afbe-7dfce61fb216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        }
      },
      "source": [
        "pruning_params = {\n",
        "      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                   final_sparsity=0.90,\n",
        "                                                   begin_step=2000,\n",
        "                                                   end_step=end_step,\n",
        "                                                   frequency=100)\n",
        "}\n",
        "\n",
        "pruned_model = tf.keras.Sequential([\n",
        "    sparsity.prune_low_magnitude(\n",
        "        l.Conv2D(32, 5, padding='same', activation='relu'),\n",
        "        input_shape=input_shape,\n",
        "        **pruning_params),\n",
        "    l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
        "    l.BatchNormalization(),\n",
        "    sparsity.prune_low_magnitude(\n",
        "        l.Conv2D(64, 5, padding='same', activation='relu'), **pruning_params),\n",
        "    l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
        "    l.Flatten(),\n",
        "    sparsity.prune_low_magnitude(l.Dense(1024, activation='relu'),\n",
        "                                 **pruning_params),\n",
        "    l.Dropout(0.4),\n",
        "    sparsity.prune_low_magnitude(l.Dense(num_classes, activation='softmax'),\n",
        "                                 **pruning_params)\n",
        "])\n",
        "\n",
        "pruned_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0604 17:58:21.893443 140389961176960 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_schedule.py:240: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_conv2d_2 (None, 28, 28, 32)        1634      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 32)        128       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_3 (None, 14, 14, 64)        102466    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_2  (None, 1024)              6423554   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_3  (None, 10)                20492     \n",
            "=================================================================\n",
            "Total params: 6,548,274\n",
            "Trainable params: 3,274,698\n",
            "Non-trainable params: 3,273,576\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YczppQ6vEPJg"
      },
      "source": [
        "Load Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eIO8UpZyEYkp",
        "outputId": "f532a86e-ceb0-4ca5-b102-dac623cb2a97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "logdir = tempfile.mkdtemp()\n",
        "print('Writing training logs to ' + logdir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing training logs to /tmp/tmppt5se9k7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KKb8XpDkA8TN",
        "outputId": "e740dcad-87c2-43f0-c8ea-98f96d9a76f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorboard --logdir={logdir}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%tensorboard` not found.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z2166laKE_N6"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Start pruning from step 2000 when accuracy >98%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GGoOTwQRzEP4",
        "outputId": "3aaa174f-bbd8-458a-803c-55413428112c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "pruned_model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "# Add a pruning step callback to peg the pruning step to the optimizer's\n",
        "# step. Also add a callback to add pruning summaries to tensorboard\n",
        "callbacks = [\n",
        "    sparsity.UpdatePruningStep(),\n",
        "    sparsity.PruningSummaries(log_dir=logdir, profile_batch=0)\n",
        "]\n",
        "\n",
        "pruned_model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=10,\n",
        "          verbose=1,\n",
        "          callbacks=callbacks,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score = pruned_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 7s 110us/sample - loss: 0.1990 - acc: 0.9497 - val_loss: 0.0925 - val_acc: 0.9860\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 5s 89us/sample - loss: 0.0473 - acc: 0.9857 - val_loss: 0.0288 - val_acc: 0.9907\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 5s 90us/sample - loss: 0.0342 - acc: 0.9890 - val_loss: 0.0291 - val_acc: 0.9905\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 5s 89us/sample - loss: 0.0241 - acc: 0.9923 - val_loss: 0.0299 - val_acc: 0.9911\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 5s 92us/sample - loss: 0.0198 - acc: 0.9936 - val_loss: 0.0206 - val_acc: 0.9925\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 6s 92us/sample - loss: 0.0155 - acc: 0.9953 - val_loss: 0.0202 - val_acc: 0.9923\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 5s 91us/sample - loss: 0.0126 - acc: 0.9959 - val_loss: 0.0222 - val_acc: 0.9927\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 5s 91us/sample - loss: 0.0133 - acc: 0.9955 - val_loss: 0.0257 - val_acc: 0.9914\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 6s 92us/sample - loss: 0.0145 - acc: 0.9953 - val_loss: 0.0279 - val_acc: 0.9915\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 5s 91us/sample - loss: 0.0128 - acc: 0.9959 - val_loss: 0.0202 - val_acc: 0.9943\n",
            "Test loss: 0.020153377860183536\n",
            "Test accuracy: 0.9943\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqB2SJFSssLi",
        "colab_type": "code",
        "outputId": "bf7a3e24-4972-4a09-ed21-82cdf29f0adf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%timeit pruned_model.evaluate(x_train, y_train, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 6.04 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8Nzrm5pN1viP"
      },
      "source": [
        "### Save and restore the pruned model\n",
        "\n",
        "Continue training for two epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rd8G7srV2dcI",
        "outputId": "e0d9a5a0-418c-4b6e-b399-d6c5748d7e65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "_, checkpoint_file = tempfile.mkstemp('.h5')\n",
        "print('Saving pruned model to: ', checkpoint_file)\n",
        "# saved_model() sets include_optimizer to True by default. Spelling it out here\n",
        "# to highlight.\n",
        "tf.keras.models.save_model(pruned_model, checkpoint_file, include_optimizer=True)\n",
        "\n",
        "with sparsity.prune_scope():\n",
        "  restored_model = tf.keras.models.load_model(checkpoint_file)\n",
        "\n",
        "restored_model.fit(x_train, y_train,\n",
        "                   batch_size=batch_size,\n",
        "                   epochs=2,\n",
        "                   verbose=1,\n",
        "                   callbacks=callbacks,\n",
        "                   validation_data=(x_test, y_test))\n",
        "\n",
        "score = restored_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving pruned model to:  /tmp/tmpremqp07e.h5\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "60000/60000 [==============================] - 7s 111us/sample - loss: 0.0111 - acc: 0.9967 - val_loss: 0.0228 - val_acc: 0.9928\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 5s 89us/sample - loss: 0.0068 - acc: 0.9979 - val_loss: 0.0243 - val_acc: 0.9921\n",
            "Test loss: 0.02432157191496144\n",
            "Test accuracy: 0.9921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1vV7pZrW5TQh"
      },
      "source": [
        "In the example above, a few things to note are:\n",
        "\n",
        "\n",
        "*   When saving the model, include_optimizer must be set to True. We need to preserve the state of the optimizer across training sessions for pruning to work properly.\n",
        "*   When loading the pruned model, you need the prune_scope() for deseriazliation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tMTFhyc0vAA3"
      },
      "source": [
        "### Strip the pruning wrappers from the pruned model before export for serving\n",
        "Before exporting a serving model, you'd need to call the `strip_pruning` API to strip the pruning wrappers from the model, as it's only needed for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jyCjjpUjvImz",
        "outputId": "4c6a1964-018e-4ab2-9068-4ed378bc3e98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "final_model = sparsity.strip_pruning(pruned_model)\n",
        "final_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 32)        832       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 14, 14, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1024)              3212288   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 3,274,762\n",
            "Trainable params: 3,274,698\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzmGZyDIu896",
        "colab_type": "code",
        "outputId": "6d3025e0-5ead-4c73-aae0-79b9fe67eaaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "final_model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "%timeit final_model.evaluate(x_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0040 - acc: 0.9993\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0040 - acc: 0.9993\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0040 - acc: 0.9993\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0040 - acc: 0.9993\n",
            "1 loop, best of 3: 3.88 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B63tViKp_qLK",
        "outputId": "ac4a8992-a694-4ec8-fe3d-7b0ee231f9fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
        "print('Saving pruned model to: ', pruned_keras_file)\n",
        "\n",
        "# No need to save the optimizer with the graph for serving.\n",
        "tf.keras.models.save_model(final_model, pruned_keras_file, include_optimizer=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving pruned model to:  /tmp/tmpxx2rv58y.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GhXHuBAGOBvY"
      },
      "source": [
        "### Compare the size of the unpruned vs. pruned model after compression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hk4DoZTIy2uU",
        "outputId": "eb40b80d-a63d-4b4f-8b8d-58c17ac25797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "_, zip1 = tempfile.mkstemp('.zip') \n",
        "with zipfile.ZipFile(zip1, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "  f.write(keras_file)\n",
        "print(\"Size of the unpruned model before compression: %.2f Mb\" % \n",
        "      (os.path.getsize(keras_file) / float(2**20)))\n",
        "print(\"Size of the unpruned model after compression: %.2f Mb\" % \n",
        "      (os.path.getsize(zip1) / float(2**20)))\n",
        "\n",
        "_, zip2 = tempfile.mkstemp('.zip') \n",
        "with zipfile.ZipFile(zip2, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "  f.write(pruned_keras_file)\n",
        "print(\"Size of the pruned model before compression: %.2f Mb\" % \n",
        "      (os.path.getsize(pruned_keras_file) / float(2**20)))\n",
        "print(\"Size of the pruned model after compression: %.2f Mb\" % \n",
        "      (os.path.getsize(zip2) / float(2**20)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the unpruned model before compression: 12.52 Mb\n",
            "Size of the unpruned model after compression: 11.59 Mb\n",
            "Size of the pruned model before compression: 12.52 Mb\n",
            "Size of the pruned model after compression: 2.51 Mb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dayb_w_GqWs_"
      },
      "source": [
        "### Prune a whole model\n",
        "\n",
        "The `prune_low_magnitude` function can also be applied to the entire Keras model. \n",
        "\n",
        "In this case, the algorithm will be applied to all layers that are ameanable to weight pruning (that the API knows about). Layers that the API knows are not ameanable to weight pruning will be ignored, and unknown layers to the API will cause an error.\n",
        "\n",
        "*If your model has layers that the API does not know how to prune their weights, but are perfectly fine to leave \"un-pruned\", then just apply the API in a per-layer basis.*\n",
        "\n",
        "Regarding pruning configuration, the same settings apply to all prunable layers in the model.\n",
        "\n",
        "Also noteworthy is that pruning doesn't preserve the optimizer associated with the original model. As a result, it is necessary to re-compile the pruned model with a new optimizer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I-W7Sj8fZjeb"
      },
      "source": [
        "Before we move forward with the example, lets address the common use case where you may already have a serialized pre-trained Keras model, which you would like to apply weight pruning on. We will take the original MNIST model trained previously to show how this works. In this case, you start by loading the model into memory like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qJm1SJxfqy2e",
        "outputId": "13dda270-cca8-40cb-aec0-a38c8c0344c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Load the serialized model\n",
        "loaded_model = tf.keras.models.load_model(keras_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0604 18:17:38.932591 140389961176960 hdf5_format.py:193] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "etYrWnrSMpB5"
      },
      "source": [
        "Then you can prune the model loaded and compile the pruned model for training. In this case training will restart from step 0. Given the model we loadded already reached a satisfactory accuracy, we can start pruning immediately. As a result, we set the begin_step to 0 here, and only train for another four epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CabnOldzrSN2",
        "outputId": "5403d909-589d-4c16-c1f4-cd6c529b7cb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "epochs = 4\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print(end_step)\n",
        "\n",
        "new_pruning_params = {\n",
        "      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                   final_sparsity=0.90,\n",
        "                                                   begin_step=0,\n",
        "                                                   end_step=end_step,\n",
        "                                                   frequency=100)\n",
        "}\n",
        "\n",
        "new_pruned_model = sparsity.prune_low_magnitude(model, **new_pruning_params)\n",
        "new_pruned_model.summary()\n",
        "\n",
        "new_pruned_model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1876\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_conv2d ( (None, 28, 28, 32)        1634      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 14, 14, 32)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 14, 14, 32)        129       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 14, 14, 64)        102466    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 7, 7, 64)          1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_flatten  (None, 3136)              1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense (P (None, 1024)              6423554   \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout  (None, 1024)              1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_1  (None, 10)                20492     \n",
            "=================================================================\n",
            "Total params: 6,548,279\n",
            "Trainable params: 3,274,698\n",
            "Non-trainable params: 3,273,581\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9qCipfCnaY7g"
      },
      "source": [
        "Load tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VPBlcTXWB9tx",
        "outputId": "5fcd7f76-505e-4cff-a515-4e5afbe176a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "logdir = tempfile.mkdtemp()\n",
        "print('Writing training logs to ' + logdir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing training logs to /tmp/tmpyo_esn74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X43ix4NZCDNS",
        "outputId": "5b99fbb8-dfa4-4ac5-fdbd-def8d4189947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorboard --logdir={logdir}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%tensorboard` not found.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r2hLPO7KNKq_"
      },
      "source": [
        "### Train the model for another four epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "36hymxokrnbw",
        "outputId": "e9361766-1124-46f6-b0c6-d1453312eb9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "# Add a pruning step callback to peg the pruning step to the optimizer's\n",
        "# step. Also add a callback to add pruning summaries to tensorboard\n",
        "callbacks = [\n",
        "    sparsity.UpdatePruningStep(),\n",
        "    sparsity.PruningSummaries(log_dir=logdir, profile_batch=0)\n",
        "]\n",
        "\n",
        "new_pruned_model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          callbacks=callbacks,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score = new_pruned_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/4\n",
            "60000/60000 [==============================] - 8s 127us/sample - loss: 0.0126 - acc: 0.9959 - val_loss: 0.0250 - val_acc: 0.9931\n",
            "Epoch 2/4\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.0161 - acc: 0.9948 - val_loss: 0.0247 - val_acc: 0.9921\n",
            "Epoch 3/4\n",
            "60000/60000 [==============================] - 6s 103us/sample - loss: 0.0197 - acc: 0.9937 - val_loss: 0.0304 - val_acc: 0.9897\n",
            "Epoch 4/4\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.0179 - acc: 0.9941 - val_loss: 0.0256 - val_acc: 0.9916\n",
            "Test loss: 0.025621396088873736\n",
            "Test accuracy: 0.9916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSd2HUS7tkcX",
        "colab_type": "code",
        "outputId": "3910f294-af0d-4484-a9cf-fa5f6a4cde90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%timeit new_pruned_model.evaluate(x_train, y_train, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 7.16 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z0yphbuRarfT"
      },
      "source": [
        "### Export the pruned model for serving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "anFHUpCXrxMe",
        "outputId": "d4320b30-1468-4d5c-c9d3-436b4d929934",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "final_model = sparsity.strip_pruning(pruned_model)\n",
        "final_model.summary()\n",
        "final_model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 32)        832       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 14, 14, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1024)              3212288   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 3,274,762\n",
            "Trainable params: 3,274,698\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsXsbgJ1ty8l",
        "colab_type": "code",
        "outputId": "5fd0d6b7-b0ae-43cb-a021-239e8bbfc59d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%timeit final_model.evaluate(x_train, y_train, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 3.69 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4CmEtxHEso7g",
        "outputId": "b05bf619-d177-4a87-b176-279ceb919a00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "_, new_pruned_keras_file = tempfile.mkstemp('.h5')\n",
        "print('Saving pruned model to: ', new_pruned_keras_file)\n",
        "tf.keras.models.save_model(final_model, new_pruned_keras_file, \n",
        "                        include_optimizer=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving pruned model to:  /tmp/tmpdraybyoj.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YT8YqXAza3Tt"
      },
      "source": [
        "The model size after compression is the same as the one pruned layer-by-layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AtKANC0hs2RR",
        "outputId": "315af5dc-e98c-4d06-fae9-c2b0a7cbf0d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "_, zip3 = tempfile.mkstemp('.zip')\n",
        "with zipfile.ZipFile(zip3, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "  f.write(new_pruned_keras_file)\n",
        "print(\"Size of the pruned model before compression: %.2f Mb\" \n",
        "      % (os.path.getsize(new_pruned_keras_file) / float(2**20)))\n",
        "print(\"Size of the pruned model after compression: %.2f Mb\" \n",
        "      % (os.path.getsize(zip3) / float(2**20)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the pruned model before compression: 12.52 Mb\n",
            "Size of the pruned model after compression: 2.51 Mb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zXrLGUPfIwvV"
      },
      "source": [
        "## Convert to TensorFlow Lite\n",
        "\n",
        "Finally, you can convert the pruned model to a format that's runnable on your targeting backend. Tensorflow Lite is an example format you can use to deploy to mobile devices. To convert to a Tensorflow Lite graph, you need to use the TFLiteConverter as below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1f9Eb2K0bcJG"
      },
      "source": [
        "### Convert the model with TFLiteConverter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ctqfiix-H-x7",
        "outputId": "a4f25924-e67d-4602-b74c-1449dad8cfd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "tflite_model_file = '/tmp/sparse_mnist.tflite'\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model_file(pruned_keras_file)\n",
        "tflite_model = converter.convert()\n",
        "with open(tflite_model_file, 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0604 18:31:03.228424 140389961176960 hdf5_format.py:193] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fdIiNnrPANcw"
      },
      "source": [
        "### Size of the TensorFlow Lite model after compression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iYMSXAU1AUYI",
        "outputId": "7513dbf7-a772-411c-92bf-a8d5424fb8ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "_, zip_tflite = tempfile.mkstemp('.zip')\n",
        "with zipfile.ZipFile(zip_tflite, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "  f.write(tflite_model_file)\n",
        "print(\"Size of the tflite model before compression: %.2f Mb\" \n",
        "      % (os.path.getsize(tflite_model_file) / float(2**20)))\n",
        "print(\"Size of the tflite model after compression: %.2f Mb\" \n",
        "      % (os.path.getsize(zip_tflite) / float(2**20)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the tflite model before compression: 12.49 Mb\n",
            "Size of the tflite model after compression: 2.44 Mb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vBqUX1qopV1k"
      },
      "source": [
        "### Evaluate the accuracy of the TensorFlow Lite model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F5AY-TuivmbP",
        "outputId": "79d7d7af-1388-4eff-d5b7-ac66068a72fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
        "interpreter.allocate_tensors()\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "def eval_model(interpreter, x_test, y_test):\n",
        "  total_seen = 0\n",
        "  num_correct = 0\n",
        "\n",
        "  for img, label in zip(x_test, y_test):\n",
        "    inp = img.reshape((1, 28, 28, 1))\n",
        "    total_seen += 1\n",
        "    interpreter.set_tensor(input_index, inp)\n",
        "    interpreter.invoke()\n",
        "    predictions = interpreter.get_tensor(output_index)\n",
        "    if np.argmax(predictions) == np.argmax(label):\n",
        "      num_correct += 1\n",
        "\n",
        "    if total_seen % 1000 == 0:\n",
        "        print(\"Accuracy after %i images: %f\" %\n",
        "              (total_seen, float(num_correct) / float(total_seen)))\n",
        "\n",
        "  return float(num_correct) / float(total_seen)\n",
        "\n",
        "print(eval_model(interpreter, x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy after 1000 images: 0.994000\n",
            "Accuracy after 2000 images: 0.992000\n",
            "Accuracy after 3000 images: 0.989333\n",
            "Accuracy after 4000 images: 0.990750\n",
            "Accuracy after 5000 images: 0.991600\n",
            "Accuracy after 6000 images: 0.992333\n",
            "Accuracy after 7000 images: 0.993000\n",
            "Accuracy after 8000 images: 0.993875\n",
            "Accuracy after 9000 images: 0.994556\n",
            "Accuracy after 10000 images: 0.994300\n",
            "0.9943\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zalu5Ng7D7xi"
      },
      "source": [
        "### Post-training quantize the TensorFlow Lite model\n",
        "\n",
        "You can combine pruning with other optimization techniques like post training quantization. As a recap, post-training quantization converts weights to 8 bit precision as part of model conversion from keras model to TFLite's flat buffer, resulting in a 4x reduction in the model size.\n",
        "\n",
        "In the following example, we take the pruned keras model, convert it with post-training quantization, check the size reduction and validate its accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wbTNqf1KER0z",
        "outputId": "ec7b3bce-3e69-40b1-a052-2ebb230f30b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model_file(pruned_keras_file)\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "tflite_quant_model_file = '/tmp/sparse_mnist_quant.tflite'\n",
        "with open(tflite_quant_model_file, 'wb') as f:\n",
        "  f.write(tflite_quant_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0604 18:31:45.182760 140389961176960 hdf5_format.py:193] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s6a8bH_-EXeR",
        "outputId": "fa67a481-216b-4318-c69b-30528ae56b56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "_, zip_tflite = tempfile.mkstemp('.zip')\n",
        "with zipfile.ZipFile(zip_tflite, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "  f.write(tflite_quant_model_file)\n",
        "print(\"Size of the tflite model before compression: %.2f Mb\" \n",
        "      % (os.path.getsize(tflite_quant_model_file) / float(2**20)))\n",
        "print(\"Size of the tflite model after compression: %.2f Mb\" \n",
        "      % (os.path.getsize(zip_tflite) / float(2**20)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the tflite model before compression: 3.13 Mb\n",
            "Size of the tflite model after compression: 0.59 Mb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lMuuxZs_QxMt"
      },
      "source": [
        "The size of the quantized model is roughly 1/4 of the orignial one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dkv9mauCEami",
        "outputId": "59eee5d0-89e3-42c0-e403-b5ab9267c37b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=str(tflite_quant_model_file))\n",
        "interpreter.allocate_tensors()\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "print(eval_model(interpreter, x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy after 1000 images: 0.994000\n",
            "Accuracy after 2000 images: 0.991500\n",
            "Accuracy after 3000 images: 0.989000\n",
            "Accuracy after 4000 images: 0.990500\n",
            "Accuracy after 5000 images: 0.991200\n",
            "Accuracy after 6000 images: 0.992000\n",
            "Accuracy after 7000 images: 0.992571\n",
            "Accuracy after 8000 images: 0.993500\n",
            "Accuracy after 9000 images: 0.994222\n",
            "Accuracy after 10000 images: 0.994000\n",
            "0.994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ygzK3KZcoZ-w"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we showed you how to create *sparse models* with the TensorFlow model optimization toolkit weight pruning API. Right now, this allows you to create models that take significant less space on disk. The resulting model can also be more efficiently implemented to avoid computation; in the future TensorFlow Lite will provide such capabilities.\n",
        "\n",
        "More specifically, we walked you through an end-to-end example of training a simple MNIST model that used the weight pruning API. We showed you how to convert it to the Tensorflow Lite format for mobile deployment, and demonstrated how with simple file compression the model size was reduced 5x.\n",
        "\n",
        "We encourage you to try this new capability on your Keras models, which can be particularly important for deployment in resource-constraint environments. \n",
        "\n"
      ]
    }
  ]
}